{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('./data/input.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "URL_ID    147\n",
       "URL       147\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'articles'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://insights.blackcoffer.com/development-of-ea-robot-for-automated-trading/'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = df.iloc[4]['URL']\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get('https://insights.blackcoffer.com/securing-sensitive-financial-data-with-privacy-preserving-machine-learning-for-predictive-analytics/')\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Development of EA Robot for Automated Trading'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = soup.find('h1', class_='entry-title').get_text(strip=True)\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Objective: The goal of this project is to build an Expert Advisor (EA) Robot that automates trading by using predictions generated from a machine learning (ML) model. The EA will operate in real-time, leveraging both historical and live data to make buy/sell decisions. Data can be obtained via two primary methods: API : A free version that provides limited data for a single currency pair, and a paid version offering access to multiple pairs. Trading Platforms : FX Pro or XM, which will serve as direct sources for real-time trading data. Additionally, the EA Robot will use an MQL5 script that defines the trading strategy, enabling automated trading on any MetaTrader 5 (MT5) platform. The core focus is to train an ML model for predictive trading, integrate data streams, and implement a scalable MQL5 strategy for execution. Key Steps: 1. Project Setup API and Platform Access : Obtain API credentials for both free and paid versions. Set up FX Pro or XM trading platforms and retrieve login details. Ensure compatibility with MetaTrader 5 (MT5) for seamless integration. Test API Connection : Test API endpoints to verify data retrieval for currency pairs. This ensures real-time data flow for both historical and live trading. 2. Data Retrieval and Preparation API Data Integration : Set up API connections to retrieve historical and live trading data. The free version will provide access to data for a single currency pair (e.g., EURUSD), while the paid version allows for multiple pairs (e.g., GBPUSD, GOLD, USDCAD, etc.). FX Pro / XM Data Handling : Integrate FX Pro and XM platforms for data reading and trade execution. Use Python and MT5 libraries to retrieve real-time data for selected currency pairs. Historical Data Storage : Collect historical data for model training, storing it in a structured format (e.g., CSV) for further processing. 3. Model Development and Training Feature Engineering : Compute technical indicators (e.g., RSI, MACD, EMA) for selected currency pairs using historical data. Generate buy/sell signals for training, ensuring the data reflects actual market patterns. Model Training : Develop and train the model using historical data to predict buy/sell signals by choosing appropriate ML model like ( XGBoost, LSTM, or Reinforcement Learning) Focus on ensuring that the model can generalize well to unseen data and respond effectively to market changes. Model Evaluation : Evaluate model performance based on metrics like accuracy, precision, recall, and profit optimization. Adjust and fine-tune hyperparameters for improved predictive performance. 4. Backtesting and Performance Evaluation Backtesting Framework Setup: Develop a system to test the model on historical data. Performance Metrics Definition: Define relevant metrics (e.g., Sharpe ratio, drawdown). Strategy Backtesting: Run the model through historical data to evaluate performance. Results Analysis: Analyze backtesting results and identify areas for improvement. 5. Real-Time Data Integration Real-Time Data Handling : Set up continuous data feeds via APIs or FX Pro/XM platforms, fetching real-time trading data at regular intervals (e.g., 15 minutes). Ensure that the data is cleaned, preprocessed, and normalized on the fly for live prediction purposes. Technical Indicators Calculation : Compute technical indicators on real-time data, ensuring the model uses up-to-date market conditions. Prediction Script : Develop scripts to apply the trained model to real-time data and generate buy/sell predictions. 6. MQL5 Script Development Strategy Implementation : Develop an MQL5 script that includes the trading strategy based on model predictions are creates an interface between the AI system and trading platform. The script will incorporate logic for stop-loss, take-profit, and position sizing, ensuring that risk management is integrated. Platform Compatibility : Ensure the MQL5 script is compatible with any MT5 platform for easy deployment. Execution of Trades : Implement a system where the EA uses the strategy to execute trades automatically based on the model’s predictions.  This approach ensures the development of a robust, data-driven EA Robot that integrates ML predictions with real-time trading platforms and APIs. It also guarantees flexibility for live trading on various currency pairs and adaptability to changing market conditions. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_div = soup.find('div', class_='td-post-content tagdiv-type')\n",
    "article_text = article_div.get_text(separator=' ', strip=True) if article_div else \"No Content\"\n",
    "article_text = article_text.replace('_', '')\n",
    "article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Objective Objective The goal of this project is to build an Expert Advisor EA Robot that automates trading by using predictions generated from a machine learning ML model The EA will operate in realtime leveraging both historical and live data to make buysell decisions Data can be obtained via two primary methods API A free version that provides limited data for a single currency pair and a paid version offering access to multiple pairsTrading Platforms FX Pro or XM which will serve as direct sources for realtime trading data API A free version that provides limited data for a single currency pair and a paid version offering access to multiple pairs API Trading Platforms FX Pro or XM which will serve as direct sources for realtime trading data Trading Platforms Additionally the EA Robot will use an MQL5 script that defines the trading strategy enabling automated trading on any MetaTrader 5 MT5 platform The core focus is to train an ML model for predictive trading integrate data streams and implement a scalable MQL5 strategy for execution Key Steps Key Steps 1 Project Setup 1 Project Setup API and Platform AccessObtain API credentials for both free and paid versionsSet up FX Pro or XM trading platforms and retrieve login detailsEnsure compatibility with MetaTrader 5 MT5 for seamless integrationTest API ConnectionTest API endpoints to verify data retrieval for currency pairs This ensures realtime data flow for both historical and live trading API and Platform AccessObtain API credentials for both free and paid versionsSet up FX Pro or XM trading platforms and retrieve login detailsEnsure compatibility with MetaTrader 5 MT5 for seamless integration API and Platform Access Obtain API credentials for both free and paid versionsSet up FX Pro or XM trading platforms and retrieve login detailsEnsure compatibility with MetaTrader 5 MT5 for seamless integration Obtain API credentials for both free and paid versions Set up FX Pro or XM trading platforms and retrieve login details Ensure compatibility with MetaTrader 5 MT5 for seamless integration Test API ConnectionTest API endpoints to verify data retrieval for currency pairs This ensures realtime data flow for both historical and live trading Test API Connection Test API endpoints to verify data retrieval for currency pairs This ensures realtime data flow for both historical and live trading Test API endpoints to verify data retrieval for currency pairs This ensures realtime data flow for both historical and live trading 2 Data Retrieval and Preparation 2 Data Retrieval and Preparation API Data IntegrationSet up API connections to retrieve historical and live trading dataThe free version will provide access to data for a single currency pair eg EURUSD while the paid version allows for multiple pairs eg GBPUSD GOLD USDCAD etcFX Pro  XM Data HandlingIntegrate FX Pro and XM platforms for data reading and trade executionUse Python and MT5 libraries to retrieve realtime data for selected currency pairsHistorical Data StorageCollect historical data for model training storing it in a structured format eg CSV for further processing API Data IntegrationSet up API connections to retrieve historical and live trading dataThe free version will provide access to data for a single currency pair eg EURUSD while the paid version allows for multiple pairs eg GBPUSD GOLD USDCAD etc API Data Integration Set up API connections to retrieve historical and live trading dataThe free version will provide access to data for a single currency pair eg EURUSD while the paid version allows for multiple pairs eg GBPUSD GOLD USDCAD etc Set up API connections to retrieve historical and live trading data The free version will provide access to data for a single currency pair eg EURUSD while the paid version allows for multiple pairs eg GBPUSD GOLD USDCAD etc FX Pro  XM Data HandlingIntegrate FX Pro and XM platforms for data reading and trade executionUse Python and MT5 libraries to retrieve realtime data for selected currency pairs FX Pro  XM Data Handling Integrate FX Pro and XM platforms for data reading and trade executionUse Python and MT5 libraries to retrieve realtime data for selected currency pairs Integrate FX Pro and XM platforms for data reading and trade execution Use Python and MT5 libraries to retrieve realtime data for selected currency pairs Historical Data StorageCollect historical data for model training storing it in a structured format eg CSV for further processing Historical Data Storage Collect historical data for model training storing it in a structured format eg CSV for further processing Collect historical data for model training storing it in a structured format eg CSV for further processing 3 Model Development and Training 3 Model Development and Training Feature EngineeringCompute technical indicators eg RSI MACD EMA for selected currency pairs using historical dataGenerate buysell signals for training ensuring the data reflects actual market patternsModel TrainingDevelop and train the model using historical data to predict buysell signals by choosing appropriate ML model like  XGBoost LSTM or Reinforcement LearningFocus on ensuring that the model can generalize well to unseen data and respond effectively to market changesModel EvaluationEvaluate model performance based on metrics like accuracy precision recall and profit optimizationAdjust and finetune hyperparameters for improved predictive performance Feature EngineeringCompute technical indicators eg RSI MACD EMA for selected currency pairs using historical dataGenerate buysell signals for training ensuring the data reflects actual market patterns Feature Engineering Compute technical indicators eg RSI MACD EMA for selected currency pairs using historical dataGenerate buysell signals for training ensuring the data reflects actual market patterns Compute technical indicators eg RSI MACD EMA for selected currency pairs using historical data Generate buysell signals for training ensuring the data reflects actual market patterns Model TrainingDevelop and train the model using historical data to predict buysell signals by choosing appropriate ML model like  XGBoost LSTM or Reinforcement LearningFocus on ensuring that the model can generalize well to unseen data and respond effectively to market changes Model Training Develop and train the model using historical data to predict buysell signals by choosing appropriate ML model like  XGBoost LSTM or Reinforcement LearningFocus on ensuring that the model can generalize well to unseen data and respond effectively to market changes Develop and train the model using historical data to predict buysell signals by choosing appropriate ML model like  XGBoost LSTM or Reinforcement Learning Focus on ensuring that the model can generalize well to unseen data and respond effectively to market changes Model EvaluationEvaluate model performance based on metrics like accuracy precision recall and profit optimizationAdjust and finetune hyperparameters for improved predictive performance Model Evaluation Evaluate model performance based on metrics like accuracy precision recall and profit optimizationAdjust and finetune hyperparameters for improved predictive performance Evaluate model performance based on metrics like accuracy precision recall and profit optimization Adjust and finetune hyperparameters for improved predictive performance 4 Backtesting and Performance Evaluation 4 Backtesting and Performance Evaluation Backtesting Framework SetupDevelop a system to test the model on historical dataPerformance Metrics DefinitionDefine relevant metrics eg Sharpe ratio drawdownStrategy BacktestingRun the model through historical data to evaluate performanceResults AnalysisAnalyze backtesting results and identify areas for improvement Backtesting Framework Setup Backtesting Framework Setup Develop a system to test the model on historical data Performance Metrics Definition Performance Metrics Definition Define relevant metrics eg Sharpe ratio drawdown Strategy Backtesting Strategy Backtesting Run the model through historical data to evaluate performance Results Analysis Results Analysis Analyze backtesting results and identify areas for improvement 5 RealTime Data Integration 5 RealTime Data Integration RealTime Data HandlingSet up continuous data feeds via APIs or FX ProXM platforms fetching realtime trading data at regular intervals eg 15 minutesEnsure that the data is cleaned preprocessed and normalized on the fly for live prediction purposesTechnical Indicators CalculationCompute technical indicators on realtime data ensuring the model uses uptodate market conditionsPrediction ScriptDevelop scripts to apply the trained model to realtime data and generate buysell predictions RealTime Data HandlingSet up continuous data feeds via APIs or FX ProXM platforms fetching realtime trading data at regular intervals eg 15 minutesEnsure that the data is cleaned preprocessed and normalized on the fly for live prediction purposes RealTime Data Handling Set up continuous data feeds via APIs or FX ProXM platforms fetching realtime trading data at regular intervals eg 15 minutesEnsure that the data is cleaned preprocessed and normalized on the fly for live prediction purposes Set up continuous data feeds via APIs or FX ProXM platforms fetching realtime trading data at regular intervals eg 15 minutes Ensure that the data is cleaned preprocessed and normalized on the fly for live prediction purposes Technical Indicators CalculationCompute technical indicators on realtime data ensuring the model uses uptodate market conditions Technical Indicators Calculation Compute technical indicators on realtime data ensuring the model uses uptodate market conditions Compute technical indicators on realtime data ensuring the model uses uptodate market conditions Prediction ScriptDevelop scripts to apply the trained model to realtime data and generate buysell predictions Prediction Script Develop scripts to apply the trained model to realtime data and generate buysell predictions Develop scripts to apply the trained model to realtime data and generate buysell predictions 6 MQL5 Script Development 6 MQL5 Script Development Strategy ImplementationDevelop an MQL5 script that includes the trading strategy based on model predictions are creates an interface between the AI system and trading platformThe script will incorporate logic for stoploss takeprofit and position sizing ensuring that risk management is integratedPlatform CompatibilityEnsure the MQL5 script is compatible with any MT5 platform for easy deploymentExecution of TradesImplement a system where the EA uses the strategy to execute trades automatically based on the models predictions Strategy ImplementationDevelop an MQL5 script that includes the trading strategy based on model predictions are creates an interface between the AI system and trading platformThe script will incorporate logic for stoploss takeprofit and position sizing ensuring that risk management is integrated Strategy Implementation Develop an MQL5 script that includes the trading strategy based on model predictions are creates an interface between the AI system and trading platformThe script will incorporate logic for stoploss takeprofit and position sizing ensuring that risk management is integrated Develop an MQL5 script that includes the trading strategy based on model predictions are creates an interface between the AI system and trading platform The script will incorporate logic for stoploss takeprofit and position sizing ensuring that risk management is integrated Platform CompatibilityEnsure the MQL5 script is compatible with any MT5 platform for easy deployment Platform Compatibility Ensure the MQL5 script is compatible with any MT5 platform for easy deployment Ensure the MQL5 script is compatible with any MT5 platform for easy deployment Execution of TradesImplement a system where the EA uses the strategy to execute trades automatically based on the models predictions Execution of Trades Implement a system where the EA uses the strategy to execute trades automatically based on the models predictions Implement a system where the EA uses the strategy to execute trades automatically based on the models predictions  This approach ensures the development of a robust datadriven EA Robot that integrates ML predictions with realtime trading platforms and APIs It also guarantees flexibility for live trading on various currency pairs and adaptability to changing market conditions           Contact Details Contact Details This solution was designed and developed by Blackcoffer TeamHere are my contact detailsFirm Name Blackcoffer Pvt LtdFirm Website wwwblackcoffercomFirm Address 42 EExtension Shaym Vihar Phase 1 New Delhi 110043Email ajayblackcoffercomSkype asbidyarthyWhatsApp 91 9717367468Telegram asbidyarthy        '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "content_elements = article_div.find_all()  # Get both <h1> and <p> tags\n",
    "article_text = ' '.join([element.get_text(strip=True) for element in content_elements])\n",
    "# Remove everything except letters and numbers\n",
    "article_text = re.sub(r'[^a-zA-Z0-9\\s]', '', article_text)  # Retain only letters and numbers\n",
    "article_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Objective:Objective:ThegoalofthisprojectistobuildanExpertAdvisor(EA)Robotthatautomatestradingbyusingpredictionsgeneratedfromamachinelearning(ML)model.TheEAwilloperateinrealtime,leveragingbothhistoricalandlivedatatomakebuyselldecisions.Datacanbeobtainedviatwoprimarymethods:API:Afreeversionthatprovideslimiteddataforasinglecurrencypair,andapaidversionofferingaccesstomultiplepairs.TradingPlatforms:FXProorXM,whichwillserveasdirectsourcesforrealtimetradingdata.API:Afreeversionthatprovideslimiteddataforasinglecurrencypair,andapaidversionofferingaccesstomultiplepairs.APITradingPlatforms:FXProorXM,whichwillserveasdirectsourcesforrealtimetradingdata.TradingPlatformsAdditionally,theEARobotwilluseanMQL5scriptthatdefinesthetradingstrategy,enablingautomatedtradingonanyMetaTrader5(MT5)platform.ThecorefocusistotrainanMLmodelforpredictivetrading,integratedatastreams,andimplementascalableMQL5strategyforexecution.KeySteps:KeySteps:1.ProjectSetup1.ProjectSetupAPIandPlatformAccess:ObtainAPIcredentialsforbothfreeandpaidversions.SetupFXProorXMtradingplatformsandretrievelogindetails.EnsurecompatibilitywithMetaTrader5(MT5)forseamlessintegration.TestAPIConnection:TestAPIendpointstoverifydataretrievalforcurrencypairs.Thisensuresrealtimedataflowforbothhistoricalandlivetrading.APIandPlatformAccess:ObtainAPIcredentialsforbothfreeandpaidversions.SetupFXProorXMtradingplatformsandretrievelogindetails.EnsurecompatibilitywithMetaTrader5(MT5)forseamlessintegration.APIandPlatformAccessObtainAPIcredentialsforbothfreeandpaidversions.SetupFXProorXMtradingplatformsandretrievelogindetails.EnsurecompatibilitywithMetaTrader5(MT5)forseamlessintegration.ObtainAPIcredentialsforbothfreeandpaidversions.SetupFXProorXMtradingplatformsandretrievelogindetails.EnsurecompatibilitywithMetaTrader5(MT5)forseamlessintegration.TestAPIConnection:TestAPIendpointstoverifydataretrievalforcurrencypairs.Thisensuresrealtimedataflowforbothhistoricalandlivetrading.TestAPIConnectionTestAPIendpointstoverifydataretrievalforcurrencypairs.Thisensuresrealtimedataflowforbothhistoricalandlivetrading.TestAPIendpointstoverifydataretrievalforcurrencypairs.Thisensuresrealtimedataflowforbothhistoricalandlivetrading.2.DataRetrievalandPreparation2.DataRetrievalandPreparationAPIDataIntegration:SetupAPIconnectionstoretrievehistoricalandlivetradingdata.Thefreeversionwillprovideaccesstodataforasinglecurrencypair(e.g.,EURUSD),whilethepaidversionallowsformultiplepairs(e.g.,GBPUSD,GOLD,USDCAD,etc.).FXProXMDataHandling:IntegrateFXProandXMplatformsfordatareadingandtradeexecution.UsePythonandMT5librariestoretrieverealtimedataforselectedcurrencypairs.HistoricalDataStorage:Collecthistoricaldataformodeltraining,storingitinastructuredformat(e.g.,CSV)forfurtherprocessing.APIDataIntegration:SetupAPIconnectionstoretrievehistoricalandlivetradingdata.Thefreeversionwillprovideaccesstodataforasinglecurrencypair(e.g.,EURUSD),whilethepaidversionallowsformultiplepairs(e.g.,GBPUSD,GOLD,USDCAD,etc.).APIDataIntegrationSetupAPIconnectionstoretrievehistoricalandlivetradingdata.Thefreeversionwillprovideaccesstodataforasinglecurrencypair(e.g.,EURUSD),whilethepaidversionallowsformultiplepairs(e.g.,GBPUSD,GOLD,USDCAD,etc.).SetupAPIconnectionstoretrievehistoricalandlivetradingdata.Thefreeversionwillprovideaccesstodataforasinglecurrencypair(e.g.,EURUSD),whilethepaidversionallowsformultiplepairs(e.g.,GBPUSD,GOLD,USDCAD,etc.).FXProXMDataHandling:IntegrateFXProandXMplatformsfordatareadingandtradeexecution.UsePythonandMT5librariestoretrieverealtimedataforselectedcurrencypairs.FXProXMDataHandlingIntegrateFXProandXMplatformsfordatareadingandtradeexecution.UsePythonandMT5librariestoretrieverealtimedataforselectedcurrencypairs.IntegrateFXProandXMplatformsfordatareadingandtradeexecution.UsePythonandMT5librariestoretrieverealtimedataforselectedcurrencypairs.HistoricalDataStorage:Collecthistoricaldataformodeltraining,storingitinastructuredformat(e.g.,CSV)forfurtherprocessing.HistoricalDataStorageCollecthistoricaldataformodeltraining,storingitinastructuredformat(e.g.,CSV)forfurtherprocessing.Collecthistoricaldataformodeltraining,storingitinastructuredformat(e.g.,CSV)forfurtherprocessing.3.ModelDevelopmentandTraining3.ModelDevelopmentandTrainingFeatureEngineering:Computetechnicalindicators(e.g.,RSI,MACD,EMA)forselectedcurrencypairsusinghistoricaldata.Generatebuysellsignalsfortraining,ensuringthedatareflectsactualmarketpatterns.ModelTraining:DevelopandtrainthemodelusinghistoricaldatatopredictbuysellsignalsbychoosingappropriateMLmodellike(XGBoost,LSTM,orReinforcementLearning)Focusonensuringthatthemodelcangeneralizewelltounseendataandrespondeffectivelytomarketchanges.ModelEvaluation:Evaluatemodelperformancebasedonmetricslikeaccuracy,precision,recall,andprofitoptimization.Adjustandfinetunehyperparametersforimprovedpredictiveperformance.FeatureEngineering:Computetechnicalindicators(e.g.,RSI,MACD,EMA)forselectedcurrencypairsusinghistoricaldata.Generatebuysellsignalsfortraining,ensuringthedatareflectsactualmarketpatterns.FeatureEngineeringComputetechnicalindicators(e.g.,RSI,MACD,EMA)forselectedcurrencypairsusinghistoricaldata.Generatebuysellsignalsfortraining,ensuringthedatareflectsactualmarketpatterns.Computetechnicalindicators(e.g.,RSI,MACD,EMA)forselectedcurrencypairsusinghistoricaldata.Generatebuysellsignalsfortraining,ensuringthedatareflectsactualmarketpatterns.ModelTraining:DevelopandtrainthemodelusinghistoricaldatatopredictbuysellsignalsbychoosingappropriateMLmodellike(XGBoost,LSTM,orReinforcementLearning)Focusonensuringthatthemodelcangeneralizewelltounseendataandrespondeffectivelytomarketchanges.ModelTrainingDevelopandtrainthemodelusinghistoricaldatatopredictbuysellsignalsbychoosingappropriateMLmodellike(XGBoost,LSTM,orReinforcementLearning)Focusonensuringthatthemodelcangeneralizewelltounseendataandrespondeffectivelytomarketchanges.DevelopandtrainthemodelusinghistoricaldatatopredictbuysellsignalsbychoosingappropriateMLmodellike(XGBoost,LSTM,orReinforcementLearning)Focusonensuringthatthemodelcangeneralizewelltounseendataandrespondeffectivelytomarketchanges.ModelEvaluation:Evaluatemodelperformancebasedonmetricslikeaccuracy,precision,recall,andprofitoptimization.Adjustandfinetunehyperparametersforimprovedpredictiveperformance.ModelEvaluationEvaluatemodelperformancebasedonmetricslikeaccuracy,precision,recall,andprofitoptimization.Adjustandfinetunehyperparametersforimprovedpredictiveperformance.Evaluatemodelperformancebasedonmetricslikeaccuracy,precision,recall,andprofitoptimization.Adjustandfinetunehyperparametersforimprovedpredictiveperformance.4.BacktestingandPerformanceEvaluation4.BacktestingandPerformanceEvaluationBacktestingFrameworkSetup:Developasystemtotestthemodelonhistoricaldata.PerformanceMetricsDefinition:Definerelevantmetrics(e.g.,Sharperatio,drawdown).StrategyBacktesting:Runthemodelthroughhistoricaldatatoevaluateperformance.ResultsAnalysis:Analyzebacktestingresultsandidentifyareasforimprovement.BacktestingFrameworkSetup:BacktestingFrameworkSetup:Developasystemtotestthemodelonhistoricaldata.PerformanceMetricsDefinition:PerformanceMetricsDefinition:Definerelevantmetrics(e.g.,Sharperatio,drawdown).StrategyBacktesting:StrategyBacktesting:Runthemodelthroughhistoricaldatatoevaluateperformance.ResultsAnalysis:ResultsAnalysis:Analyzebacktestingresultsandidentifyareasforimprovement.5.RealTimeDataIntegration5.RealTimeDataIntegrationRealTimeDataHandling:SetupcontinuousdatafeedsviaAPIsorFXProXMplatforms,fetchingrealtimetradingdataatregularintervals(e.g.,15minutes).Ensurethatthedataiscleaned,preprocessed,andnormalizedontheflyforlivepredictionpurposes.TechnicalIndicatorsCalculation:Computetechnicalindicatorsonrealtimedata,ensuringthemodelusesuptodatemarketconditions.PredictionScript:Developscriptstoapplythetrainedmodeltorealtimedataandgeneratebuysellpredictions.RealTimeDataHandling:SetupcontinuousdatafeedsviaAPIsorFXProXMplatforms,fetchingrealtimetradingdataatregularintervals(e.g.,15minutes).Ensurethatthedataiscleaned,preprocessed,andnormalizedontheflyforlivepredictionpurposes.RealTimeDataHandlingSetupcontinuousdatafeedsviaAPIsorFXProXMplatforms,fetchingrealtimetradingdataatregularintervals(e.g.,15minutes).Ensurethatthedataiscleaned,preprocessed,andnormalizedontheflyforlivepredictionpurposes.SetupcontinuousdatafeedsviaAPIsorFXProXMplatforms,fetchingrealtimetradingdataatregularintervals(e.g.,15minutes).Ensurethatthedataiscleaned,preprocessed,andnormalizedontheflyforlivepredictionpurposes.TechnicalIndicatorsCalculation:Computetechnicalindicatorsonrealtimedata,ensuringthemodelusesuptodatemarketconditions.TechnicalIndicatorsCalculationComputetechnicalindicatorsonrealtimedata,ensuringthemodelusesuptodatemarketconditions.Computetechnicalindicatorsonrealtimedata,ensuringthemodelusesuptodatemarketconditions.PredictionScript:Developscriptstoapplythetrainedmodeltorealtimedataandgeneratebuysellpredictions.PredictionScriptDevelopscriptstoapplythetrainedmodeltorealtimedataandgeneratebuysellpredictions.Developscriptstoapplythetrainedmodeltorealtimedataandgeneratebuysellpredictions.6.MQL5ScriptDevelopment6.MQL5ScriptDevelopmentStrategyImplementation:DevelopanMQL5scriptthatincludesthetradingstrategybasedonmodelpredictionsarecreatesaninterfacebetweentheAIsystemandtradingplatform.Thescriptwillincorporatelogicforstoploss,takeprofit,andpositionsizing,ensuringthatriskmanagementisintegrated.PlatformCompatibility:EnsuretheMQL5scriptiscompatiblewithanyMT5platformforeasydeployment.ExecutionofTrades:ImplementasystemwheretheEAusesthestrategytoexecutetradesautomaticallybasedonthemodelspredictions.StrategyImplementation:DevelopanMQL5scriptthatincludesthetradingstrategybasedonmodelpredictionsarecreatesaninterfacebetweentheAIsystemandtradingplatform.Thescriptwillincorporatelogicforstoploss,takeprofit,andpositionsizing,ensuringthatriskmanagementisintegrated.StrategyImplementationDevelopanMQL5scriptthatincludesthetradingstrategybasedonmodelpredictionsarecreatesaninterfacebetweentheAIsystemandtradingplatform.Thescriptwillincorporatelogicforstoploss,takeprofit,andpositionsizing,ensuringthatriskmanagementisintegrated.DevelopanMQL5scriptthatincludesthetradingstrategybasedonmodelpredictionsarecreatesaninterfacebetweentheAIsystemandtradingplatform.Thescriptwillincorporatelogicforstoploss,takeprofit,andpositionsizing,ensuringthatriskmanagementisintegrated.PlatformCompatibility:EnsuretheMQL5scriptiscompatiblewithanyMT5platformforeasydeployment.PlatformCompatibilityEnsuretheMQL5scriptiscompatiblewithanyMT5platformforeasydeployment.EnsuretheMQL5scriptiscompatiblewithanyMT5platformforeasydeployment.ExecutionofTrades:ImplementasystemwheretheEAusesthestrategytoexecutetradesautomaticallybasedonthemodelspredictions.ExecutionofTradesImplementasystemwheretheEAusesthestrategytoexecutetradesautomaticallybasedonthemodelspredictions.ImplementasystemwheretheEAusesthestrategytoexecutetradesautomaticallybasedonthemodelspredictions.Thisapproachensuresthedevelopmentofarobust,datadrivenEARobotthatintegratesMLpredictionswithrealtimetradingplatformsandAPIs.Italsoguaranteesflexibilityforlivetradingonvariouscurrencypairsandadaptabilitytochangingmarketconditions.ContactDetailsContactDetailsThissolutionwasdesignedanddevelopedbyBlackcofferTeamHerearemycontactdetails:FirmName:BlackcofferPvt.Ltd.FirmWebsite:www.blackcoffer.comFirmAddress:42,EExtension,ShaymViharPhase1,NewDelhi110043Email:ajayblackcoffer.comSkype:asbidyarthyWhatsApp:919717367468Telegram:asbidyarthy'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_elements = article_div.find_all()  # Get both <h1> and <p> tags\n",
    "article_text = ' '.join([element.get_text(strip=True) for element in content_elements])\n",
    "\n",
    "# Remove anything that is not a letter, number, space, or punctuation\n",
    "article_text = re.sub(r'[^a-zA-Z0-9\\s\\.,!?;:()\\-]', '', article_text)  # Keep letters, numbers, spaces, and common punctuation\n",
    "article_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article Netclan20241017 to articles\\Netclan20241017.txt\n"
     ]
    }
   ],
   "source": [
    "content = f\"{title}\\n\\n{article_text}\"\n",
    "\n",
    "# Save to a text file named after URL_ID\n",
    "file_name = os.path.join('articles', f\"{df.iloc[0]['URL_ID']}.txt\")\n",
    "with open(file_name, 'w', encoding='utf-8') as file:\n",
    "    file.write(content)\n",
    "\n",
    "print(f\"Saved article {df.iloc[0]['URL_ID']} to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective: The goal of this project is to build an Expert Advisor (EA) Robot that automates trading by using predictions generated from a machine learning (ML) model. The EA will operate in real-time, leveraging both historical and live data to make buy/sell decisions. Data can be obtained via two primary methods: API : A free version that provides limited data for a single currency pair, and a paid version offering access to multiple pairs. Trading Platforms : FX Pro or XM, which will serve as direct sources for real-time trading data. Additionally, the EA Robot will use an MQL5 script that defines the trading strategy, enabling automated trading on any MetaTrader 5 (MT5) platform. The core focus is to train an ML model for predictive trading, integrate data streams, and implement a scalable MQL5 strategy for execution. Key Steps: 1. Project Setup API and Platform Access : Obtain API credentials for both free and paid versions. Set up FX Pro or XM trading platforms and retrieve login details. Ensure compatibility with MetaTrader 5 (MT5) for seamless integration. Test API Connection : Test API endpoints to verify data retrieval for currency pairs. This ensures real-time data flow for both historical and live trading. 2. Data Retrieval and Preparation API Data Integration : Set up API connections to retrieve historical and live trading data. The free version will provide access to data for a single currency pair (e.g., EURUSD), while the paid version allows for multiple pairs (e.g., GBPUSD, GOLD, USDCAD, etc.). FX Pro / XM Data Handling : Integrate FX Pro and XM platforms for data reading and trade execution. Use Python and MT5 libraries to retrieve real-time data for selected currency pairs. Historical Data Storage : Collect historical data for model training, storing it in a structured format (e.g., CSV) for further processing. 3. Model Development and Training Feature Engineering : Compute technical indicators (e.g., RSI, MACD, EMA) for selected currency pairs using historical data. Generate buy/sell signals for training, ensuring the data reflects actual market patterns. Model Training : Develop and train the model using historical data to predict buy/sell signals by choosing appropriate ML model like ( XGBoost, LSTM, or Reinforcement Learning) Focus on ensuring that the model can generalize well to unseen data and respond effectively to market changes. Model Evaluation : Evaluate model performance based on metrics like accuracy, precision, recall, and profit optimization. Adjust and fine-tune hyperparameters for improved predictive performance. 4. Backtesting and Performance Evaluation Backtesting Framework Setup: Develop a system to test the model on historical data. Performance Metrics Definition: Define relevant metrics (e.g., Sharpe ratio, drawdown). Strategy Backtesting: Run the model through historical data to evaluate performance. Results Analysis: Analyze backtesting results and identify areas for improvement. 5. Real-Time Data Integration Real-Time Data Handling : Set up continuous data feeds via APIs or FX Pro/XM platforms, fetching real-time trading data at regular intervals (e.g., 15 minutes). Ensure that the data is cleaned, preprocessed, and normalized on the fly for live prediction purposes. Technical Indicators Calculation : Compute technical indicators on real-time data, ensuring the model uses up-to-date market conditions. Prediction Script : Develop scripts to apply the trained model to real-time data and generate buy/sell predictions. 6. MQL5 Script Development Strategy Implementation : Develop an MQL5 script that includes the trading strategy based on model predictions are creates an interface between the AI system and trading platform. The script will incorporate logic for stop-loss, take-profit, and position sizing, ensuring that risk management is integrated. Platform Compatibility : Ensure the MQL5 script is compatible with any MT5 platform for easy deployment. Execution of Trades : Implement a system where the EA uses the strategy to execute trades automatically based on the model’s predictions.  This approach ensures the development of a robust, data-driven EA Robot that integrates ML predictions with real-time trading platforms and APIs. It also guarantees flexibility for live trading on various currency pairs and adaptability to changing market conditions. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy\n"
     ]
    }
   ],
   "source": [
    "# Extract the article text from the div\n",
    "if article_div:\n",
    "    article_text = article_div.get_text(separator=' ', strip=True)\n",
    "    # Remove underscores\n",
    "    article_text = article_text.replace('_', '')\n",
    "else:\n",
    "    article_text = \"No Content\"\n",
    "\n",
    "# Print or save the cleaned article text\n",
    "print(article_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article Netclan20241017 to articles\\Netclan20241017.txt\n"
     ]
    }
   ],
   "source": [
    "article_div = soup.find('div', class_='td-post-content tagdiv-type')\n",
    "article_text = article_div.get_text(separator=' ', strip=True) if article_div else \"No Content\"\n",
    "article_text = article_text.replace('_', '')\n",
    "article_text\n",
    "\n",
    "content = f\"{title}\\n\\n{article_text}\"\n",
    "\n",
    "# Save to a text file named after URL_ID\n",
    "file_name = os.path.join('articles', f\"{df.iloc[0]['URL_ID']}.txt\")\n",
    "with open(file_name, 'w', encoding='utf-8') as file:\n",
    "    file.write(content)\n",
    "\n",
    "print(f\"Saved article {df.iloc[0]['URL_ID']} to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://insights.blackcoffer.com/algorithmic-trading-for-multiple-commodities-markets-like-forex-metals-energy-etc/'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article Netclan20241017 to articles\\Netclan20241017.txt\n",
      "Saved article Netclan20241018 to articles\\Netclan20241018.txt\n",
      "Saved article Netclan20241019 to articles\\Netclan20241019.txt\n",
      "Saved article Netclan20241020 to articles\\Netclan20241020.txt\n",
      "Saved article Netclan20241021 to articles\\Netclan20241021.txt\n",
      "Saved article Netclan20241022 to articles\\Netclan20241022.txt\n",
      "Saved article Netclan20241023 to articles\\Netclan20241023.txt\n",
      "Saved article Netclan20241024 to articles\\Netclan20241024.txt\n",
      "Saved article Netclan20241025 to articles\\Netclan20241025.txt\n",
      "Saved article Netclan20241026 to articles\\Netclan20241026.txt\n",
      "Saved article Netclan20241027 to articles\\Netclan20241027.txt\n",
      "Saved article Netclan20241028 to articles\\Netclan20241028.txt\n",
      "Saved article Netclan20241029 to articles\\Netclan20241029.txt\n",
      "Saved article Netclan20241030 to articles\\Netclan20241030.txt\n",
      "Saved article Netclan20241031 to articles\\Netclan20241031.txt\n",
      "Saved article Netclan20241032 to articles\\Netclan20241032.txt\n",
      "Saved article Netclan20241033 to articles\\Netclan20241033.txt\n",
      "Saved article Netclan20241034 to articles\\Netclan20241034.txt\n",
      "Saved article Netclan20241035 to articles\\Netclan20241035.txt\n",
      "Saved article Netclan20241036 to articles\\Netclan20241036.txt\n",
      "Saved article Netclan20241037 to articles\\Netclan20241037.txt\n",
      "Saved article Netclan20241038 to articles\\Netclan20241038.txt\n",
      "Saved article Netclan20241039 to articles\\Netclan20241039.txt\n",
      "Saved article Netclan20241040 to articles\\Netclan20241040.txt\n",
      "Saved article Netclan20241041 to articles\\Netclan20241041.txt\n",
      "Saved article Netclan20241042 to articles\\Netclan20241042.txt\n",
      "Saved article Netclan20241043 to articles\\Netclan20241043.txt\n",
      "Saved article Netclan20241044 to articles\\Netclan20241044.txt\n",
      "Saved article Netclan20241045 to articles\\Netclan20241045.txt\n",
      "Saved article Netclan20241046 to articles\\Netclan20241046.txt\n",
      "Saved article Netclan20241047 to articles\\Netclan20241047.txt\n",
      "Saved article Netclan20241048 to articles\\Netclan20241048.txt\n",
      "Saved article Netclan20241049 to articles\\Netclan20241049.txt\n",
      "Saved article Netclan20241050 to articles\\Netclan20241050.txt\n",
      "Saved article Netclan20241051 to articles\\Netclan20241051.txt\n",
      "Saved article Netclan20241052 to articles\\Netclan20241052.txt\n",
      "Saved article Netclan20241053 to articles\\Netclan20241053.txt\n",
      "Saved article Netclan20241054 to articles\\Netclan20241054.txt\n",
      "Saved article Netclan20241055 to articles\\Netclan20241055.txt\n",
      "Saved article Netclan20241056 to articles\\Netclan20241056.txt\n",
      "Saved article Netclan20241057 to articles\\Netclan20241057.txt\n",
      "Saved article Netclan20241058 to articles\\Netclan20241058.txt\n",
      "Saved article Netclan20241059 to articles\\Netclan20241059.txt\n",
      "Saved article Netclan20241060 to articles\\Netclan20241060.txt\n",
      "Saved article Netclan20241061 to articles\\Netclan20241061.txt\n",
      "Saved article Netclan20241062 to articles\\Netclan20241062.txt\n",
      "Saved article Netclan20241063 to articles\\Netclan20241063.txt\n",
      "Saved article Netclan20241064 to articles\\Netclan20241064.txt\n",
      "Saved article Netclan20241065 to articles\\Netclan20241065.txt\n",
      "Saved article Netclan20241066 to articles\\Netclan20241066.txt\n",
      "Saved article Netclan20241067 to articles\\Netclan20241067.txt\n",
      "Saved article Netclan20241068 to articles\\Netclan20241068.txt\n",
      "Saved article Netclan20241069 to articles\\Netclan20241069.txt\n",
      "Saved article Netclan20241070 to articles\\Netclan20241070.txt\n",
      "Saved article Netclan20241071 to articles\\Netclan20241071.txt\n",
      "Saved article Netclan20241072 to articles\\Netclan20241072.txt\n",
      "Saved article Netclan20241073 to articles\\Netclan20241073.txt\n",
      "Saved article Netclan20241074 to articles\\Netclan20241074.txt\n",
      "Saved article Netclan20241075 to articles\\Netclan20241075.txt\n",
      "Saved article Netclan20241076 to articles\\Netclan20241076.txt\n",
      "Saved article Netclan20241077 to articles\\Netclan20241077.txt\n",
      "Saved article Netclan20241078 to articles\\Netclan20241078.txt\n",
      "Saved article Netclan20241079 to articles\\Netclan20241079.txt\n",
      "Saved article Netclan20241080 to articles\\Netclan20241080.txt\n",
      "Saved article Netclan20241081 to articles\\Netclan20241081.txt\n",
      "Saved article Netclan20241082 to articles\\Netclan20241082.txt\n",
      "Saved article Netclan20241083 to articles\\Netclan20241083.txt\n",
      "Saved article Netclan20241084 to articles\\Netclan20241084.txt\n",
      "Saved article Netclan20241085 to articles\\Netclan20241085.txt\n",
      "Saved article Netclan20241086 to articles\\Netclan20241086.txt\n",
      "Saved article Netclan20241087 to articles\\Netclan20241087.txt\n",
      "Saved article Netclan20241088 to articles\\Netclan20241088.txt\n",
      "Saved article Netclan20241089 to articles\\Netclan20241089.txt\n",
      "Saved article Netclan20241090 to articles\\Netclan20241090.txt\n",
      "Saved article Netclan20241091 to articles\\Netclan20241091.txt\n",
      "Saved article Netclan20241092 to articles\\Netclan20241092.txt\n",
      "Saved article Netclan20241093 to articles\\Netclan20241093.txt\n",
      "Saved article Netclan20241094 to articles\\Netclan20241094.txt\n",
      "Saved article Netclan20241095 to articles\\Netclan20241095.txt\n",
      "Saved article Netclan20241096 to articles\\Netclan20241096.txt\n",
      "Saved article Netclan20241097 to articles\\Netclan20241097.txt\n",
      "Saved article Netclan20241098 to articles\\Netclan20241098.txt\n",
      "Saved article Netclan20241099 to articles\\Netclan20241099.txt\n",
      "Saved article Netclan20241100 to articles\\Netclan20241100.txt\n",
      "Saved article Netclan20241101 to articles\\Netclan20241101.txt\n",
      "Saved article Netclan20241102 to articles\\Netclan20241102.txt\n",
      "Saved article Netclan20241103 to articles\\Netclan20241103.txt\n",
      "Saved article Netclan20241104 to articles\\Netclan20241104.txt\n",
      "Saved article Netclan20241105 to articles\\Netclan20241105.txt\n",
      "Saved article Netclan20241106 to articles\\Netclan20241106.txt\n",
      "Saved article Netclan20241107 to articles\\Netclan20241107.txt\n",
      "Saved article Netclan20241108 to articles\\Netclan20241108.txt\n",
      "Saved article Netclan20241109 to articles\\Netclan20241109.txt\n",
      "Saved article Netclan20241110 to articles\\Netclan20241110.txt\n",
      "Saved article Netclan20241111 to articles\\Netclan20241111.txt\n",
      "Saved article Netclan20241112 to articles\\Netclan20241112.txt\n",
      "Saved article Netclan20241113 to articles\\Netclan20241113.txt\n",
      "Saved article Netclan20241114 to articles\\Netclan20241114.txt\n",
      "Saved article Netclan20241115 to articles\\Netclan20241115.txt\n",
      "Saved article Netclan20241116 to articles\\Netclan20241116.txt\n",
      "Saved article Netclan20241117 to articles\\Netclan20241117.txt\n",
      "Saved article Netclan20241118 to articles\\Netclan20241118.txt\n",
      "Saved article Netclan20241119 to articles\\Netclan20241119.txt\n",
      "Saved article Netclan20241120 to articles\\Netclan20241120.txt\n",
      "Saved article Netclan20241121 to articles\\Netclan20241121.txt\n",
      "Saved article Netclan20241122 to articles\\Netclan20241122.txt\n",
      "Saved article Netclan20241123 to articles\\Netclan20241123.txt\n",
      "Saved article Netclan20241124 to articles\\Netclan20241124.txt\n",
      "Saved article Netclan20241125 to articles\\Netclan20241125.txt\n",
      "Saved article Netclan20241126 to articles\\Netclan20241126.txt\n",
      "Saved article Netclan20241127 to articles\\Netclan20241127.txt\n",
      "Saved article Netclan20241128 to articles\\Netclan20241128.txt\n",
      "Saved article Netclan20241129 to articles\\Netclan20241129.txt\n",
      "Saved article Netclan20241130 to articles\\Netclan20241130.txt\n",
      "Saved article Netclan20241131 to articles\\Netclan20241131.txt\n",
      "Saved article Netclan20241132 to articles\\Netclan20241132.txt\n",
      "Saved article Netclan20241133 to articles\\Netclan20241133.txt\n",
      "Saved article Netclan20241134 to articles\\Netclan20241134.txt\n",
      "Saved article Netclan20241135 to articles\\Netclan20241135.txt\n",
      "Saved article Netclan20241136 to articles\\Netclan20241136.txt\n",
      "Saved article Netclan20241137 to articles\\Netclan20241137.txt\n",
      "Saved article Netclan20241138 to articles\\Netclan20241138.txt\n",
      "Saved article Netclan20241139 to articles\\Netclan20241139.txt\n",
      "Saved article Netclan20241140 to articles\\Netclan20241140.txt\n",
      "Saved article Netclan20241141 to articles\\Netclan20241141.txt\n",
      "Saved article Netclan20241142 to articles\\Netclan20241142.txt\n",
      "Saved article Netclan20241143 to articles\\Netclan20241143.txt\n",
      "Saved article Netclan20241144 to articles\\Netclan20241144.txt\n",
      "Saved article Netclan20241145 to articles\\Netclan20241145.txt\n",
      "Saved article Netclan20241146 to articles\\Netclan20241146.txt\n",
      "Saved article Netclan20241147 to articles\\Netclan20241147.txt\n",
      "Saved article Netclan20241148 to articles\\Netclan20241148.txt\n",
      "Saved article Netclan20241149 to articles\\Netclan20241149.txt\n",
      "Saved article Netclan20241150 to articles\\Netclan20241150.txt\n",
      "Saved article Netclan20241151 to articles\\Netclan20241151.txt\n",
      "Saved article Netclan20241152 to articles\\Netclan20241152.txt\n",
      "Saved article Netclan20241153 to articles\\Netclan20241153.txt\n",
      "Saved article Netclan20241154 to articles\\Netclan20241154.txt\n",
      "Saved article Netclan20241155 to articles\\Netclan20241155.txt\n",
      "Saved article Netclan20241156 to articles\\Netclan20241156.txt\n",
      "Saved article Netclan20241157 to articles\\Netclan20241157.txt\n",
      "Saved article Netclan20241158 to articles\\Netclan20241158.txt\n",
      "Saved article Netclan20241159 to articles\\Netclan20241159.txt\n",
      "Saved article Netclan20241160 to articles\\Netclan20241160.txt\n",
      "Saved article Netclan20241161 to articles\\Netclan20241161.txt\n",
      "Saved article Netclan20241162 to articles\\Netclan20241162.txt\n",
      "Saved article Netclan20241163 to articles\\Netclan20241163.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.exceptions import Timeout, RequestException\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "\n",
    "session = requests.Session()\n",
    "retry_strategy = Retry(\n",
    "    total=3,  \n",
    "    backoff_factor=1,  # Multiplier for delay between retries: 1s, 2s, 4s, etc.\n",
    "    status_forcelist=[429, 500, 502, 503, 504], \n",
    "    allowed_methods=[\"GET\"] \n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "session.mount(\"http://\", adapter)\n",
    "session.mount(\"https://\", adapter)\n",
    "\n",
    "# Directory to save articles\n",
    "os.makedirs('articles', exist_ok=True)\n",
    "\n",
    "# Process each URL in the DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    url = row['URL']\n",
    "    url_id = row['URL_ID']\n",
    "    if url:\n",
    "        try:\n",
    "            # Send the request with a timeout\n",
    "            response = session.get(url, timeout=5)\n",
    "            response.raise_for_status() \n",
    "\n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Extract the title (if available)\n",
    "            title_element = soup.find('h1', class_='entry-title')\n",
    "            title = title_element.get_text(strip=True) if title_element else \"No Title\"\n",
    "\n",
    "            # Extract the article content (if available)\n",
    "            article_div = soup.find('div', class_='td-post-content tagdiv-type')\n",
    "            article_text = article_div.get_text(separator=' ', strip=True) if article_div else \"No Content Available\"\n",
    "\n",
    "            # Remove any underscores from the article content\n",
    "            article_text = article_text.replace('_', '')\n",
    "\n",
    "            # Combine title and article content\n",
    "            content = f\"{title}\\n\\n{article_text}\"\n",
    "\n",
    "            # Save to a text file named after URL_ID\n",
    "            file_name = os.path.join('articles', f\"{url_id}.txt\")\n",
    "            with open(file_name, 'w', encoding='utf-8') as file:\n",
    "                file.write(content)\n",
    "\n",
    "            print(f\"Saved article {url_id} to {file_name}\")\n",
    "\n",
    "        except Timeout:\n",
    "            print(f\"Timeout occurred for {url} (ID: {url_id}). Skipping.\")\n",
    "        except RequestException as e:\n",
    "            print(f\"Error fetching {url} (ID: {url_id}): {e}\")\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client Background Client: A leading IT & tech firm in the USA Industry Type: IT Products & Services: IT Consulting, IT Support, SaaS, Marketing Strategy Organization Size: 10+ The Problem Building AI and ML based YouTube analytics and content creation tool that will help youtuber to understand their subscriber’s watching behaviour, help them in content research, creation and publication. Our Solution Created a MERN stack web application and integrated AI models to helps youtuber to generated titles, descriptions, tags, hashtags, captions etc. Help them to check thumbnail quality, analysis on the videos using video auditor tool, analysis on comments using sentiments analysis, help to under their subscribers using churn predication AI model. Solution Architecture https://www.figma.com/file/WQs01mmmNBZ1SjNE2IV8Sl/Youtube-Web-App-By-SHiV?type=design&node-id=0-1&mode=design&t=Lh2jRx4bGQq6l4WU-0 Deliverables Web Applications Supports Maintenance Feature Enhancement Tech Stack Tools used VS code Language/techniques used React.js Express.js Node.js Python Models used Python libraries Skills used Data scientise Full Stack developer Databases used MongoDB Web Cloud Servers used Google Cloud Platform Project Snapshots Home Page Tool Page Dashboard Blog Page Single Blog Post About Us Contact Us Login Page Title and Description tool Page Thumbnail Quality check tool Project website url https://tubetool.ai Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy\n",
      "Client Background Client: A leading hospital chain in the USA Industry Type: Healthcare Products & Services: Healthcare solutions Organization Size: 200+ The Problem Build a web application. Develop , deploy and maintain the system in the background, and there will be more hospitals partnering with our organization utilizing this tool per our service offering, which could lead to long-term working contracts with us, if interested. The current project is a HIPPA-compliant SPA Web Application that will interface with hospital data dashboards involving discharged patients. Call-Navigator utilizes an SFTP server that is hosted in AWS, this allows our partner hospital to store patient discharge files directly to an S3 bucket. The application and dashboard are currently up and running and pulling information and data from our partner hospital. However, the project was rushed through initial development and there are several front-end features that need to be addressed (or fine-tuned/upgraded) and functions added (search/flag options and redirection) to make the application more user-friendly (dashboard accuracy). There are deliverables such as reporting that is part of the project scope that was not developed but are considered a priority after the features and functionality are improved. According to the OG developer, a GitHub account for our organization will need to be created and code uploaded to two separate projects – one for backend and one for frontend. Let me know if this project would be something you would like to further discuss. Thank you so much for your time and consideration. More information from the OG developer follows: Developer should specifically know (or be able to learn) these specific AWS features since they will be responsible for deployment and maintaining the system. For deployment, the developer must:  modify AWS security groups and target groups, etc…  push code to product **DO NOT REMOVE OG DEVELOPER ACCESS/NETWORK CONNECTION** Currently, there are GitHub actions created that will deploy the code if you set up the proper secrets, GitHub actions on FE/Angular app with deploy code to our static S3 website as well as creations invalidations for CloudFront that bypasses the caching. An ssh config link and key file are available. Need to be able to pull reports on call info by unit, by caller, by diagnosis, physician or across the hospital, specifically on flags for medications, conditions, ect. – Flags and marked issues also need to be represented on the Facility Dashboard page. – Automate the import process of file from hospital. – Set up or ability to set up Hospital employees to log in with their Blessing Credentials (single sign on). – Flags thrown need to show up somewhere in Call Navigator and also be able to go into them and view patients and issues. Right now, the only notification and way to view this is strictly through email. – Ability to hover over icons on Facility Dashboard and see info and be able to click on info and have a page with that info pop up. – On the Discharge tab, when setting a filter, we want that filter to stay set unless we change it. Right now, if you set a filter and then go into a patient, when you come back out, the filter is not set anymore and you have to set all over again. Our Solution – Need to be able to pull reports on call info by unit, by caller, by diagnosis, physician or across the hospital, specifically on flags for medications, conditions, ect. – Flags and marked issues also need to be represented on the Facility Dashboard page. – Automate the import process of file from hospital. – Set up or ability to set up Hospital employees to log in with their Blessing Credentials (single sign on). – Flags thrown need to show up somewhere in Call Navigator and also be able to go into them and view patients and issues. Right now, the only notification and way to view this is strictly through email. – Ability to hover over icons on Facility Dashboard and see info and be able to click on info and have a page with that info pop up. – On the Discharge tab, when setting a filter, we want that filter to stay set unless we change it. Right now, if you set a filter and then go into a patient, when you come back out, the filter is not set anymore and you have to set all over again. Also, let me know if there is a “scope of project” document that we need to complete to help you and your team develop a project plan and timeline. Hopefully, with the above notes and attached files, you’ll be able to put together a project plan and proposal for this project. Deliverables Fully functional application product Maintenance and support Tech Stack Tools used VS Code Language/techniques used Angular Node.js Express.js Databases used MySQL Web Cloud Servers used AWS Project Snapshots Project website url https://callsnavigator.com Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy\n",
      "Client Background Client: A leading IT & tech firm in the USA Industry Type: IT Products & Services: IT Consulting, IT Support, SaaS Organization Size: 200+ The Problem Create a ROASing dashboard to fetch google ads budget spent data using google ads api(campaign-wise). The challenge is to develop a ROAS (Return on Ad Spend) dashboard that efficiently retrieves and displays Google Ads budget spent data on a campaign-wise basis using the Google Ads API. The current system lacks a streamlined method for tracking and analyzing ad spend across various campaigns, leading to difficulties in assessing performance and optimizing budget allocation. The goal is to create a comprehensive dashboard that accurately fetches and visualizes budget data, enabling more effective analysis and decision-making. This involves integrating with the Google Ads API, ensuring data accuracy, and providing clear, actionable insights for better campaign management. 4o mini Our Solution Get Data source access a. Google ads accounts Get Access to google cloud platform a. Bigquery b. VM Setup Google Big Query Data warehouse Design db schema / models Develop Python ETL tool to pull data from source and save it to the data warehouse Deploy ETL tool to VM and run it as cron to update data at db frequently a. Check frequency to run b. Recommended 3 to 6 times a day c. The ETL tool should run automatically at the set intervals and update the db d. Add slack integration module for notifications on Cronjob failure Get Design Access of the Dashboard Implement the Dashboard a. Use Vuexy React Template b. List KPI to display in UI c. List Charts to display in the UI Implement Backend API needed to build the dashboard Solution Architecture https://drive.google.com/file/d/1ygYpSWiOQnREeOMAn4Zk5taNbWv8eaN8/view?usp=sharing Deliverables ETL Tool Dashboard Documentations Source codes Support and Maintenance Tech Stack Tools used VS Code Digital Ocean Language/techniques used Python Google Big Query Google Cloud Platform Google Ads API React.js Node.js Github Models used Python libraries (Pandas) Skills used Data Scientist Full Stack developer Databases used MySQL Web Cloud Servers used Digital Ocean What are the technical Challenges Faced during Project Execution Getting the Google Ads campaigns data How the Technical Challenges were Solved Used the google Ads APIs Project Snapshots Project website url https://roasing.com Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy\n",
      "Client Background Client: A leading automobile & tech firm in the USA Industry Type: Automobiles Products & Services: Manufacturing & Dealership, Financial Services Organization Size: 200+ The Problem The client, a Toyota dealership management firm, faced significant challenges in efficiently processing and analyzing financial data extracted from PDF files. These documents contained crucial information regarding sales, expenses, and other financial metrics across various departments, but parsing and extracting this data accurately proved to be a daunting task. The primary issues included inconsistency in PDF formatting, difficulty in table extraction, and ensuring data integrity throughout the processing pipeline. Our Solution To address these challenges, we developed a comprehensive solution tailored specifically for parsing financial data from Toyota dealership PDF documents. Our solution comprised a series of modular components, each designed to handle specific aspects of the data processing pipeline. We utilized advanced PDF parsing libraries like pdfplumber to extract tables and metadata accurately. Additionally, we implemented custom algorithms for data cleaning and validation to ensure the integrity and accuracy of the extracted data. Solution Architecture The architecture of our solution was designed with modularity and scalability in mind. It consisted of the following key components: PDF Parsing Module: Responsible for extracting tables and metadata from PDF documents using pdfplumber. Data Cleaning and Validation Module: Implemented custom algorithms to clean and validate the extracted data, ensuring consistency and accuracy. Data Aggregation and Analysis Module: Utilized pandas for aggregating and analyzing financial metrics across different departments and time periods. MongoDB Integration: Stored structured financial data in MongoDB collections for efficient storage and retrieval. Deliverables Custom Python scripts for PDF parsing and data processing tailored for Toyota dealership documents. Structured financial data stored in MongoDB collections, ensuring easy access and retrieval. Comprehensive documentation detailing system architecture, usage guidelines, and maintenance procedures. Tech Stack Tools used pdfplumber, pandas, MongoDB Language/techniques used Python, data cleaning, aggregation Models used Custom parsing algorithms Skills used Data processing, Python programming Databases used MongoDB Web Cloud Servers used GCP What are the technical Challenges Faced during Project Execution Variability in PDF document formats: Different Toyota dealership documents exhibited varying formatting styles, making consistent parsing challenging. Handling large volumes of PDF files: Processing a large number of PDF files efficiently without compromising performance was a significant challenge. Ensuring data consistency and accuracy: Maintaining data integrity throughout the processing pipeline, especially in the presence of inconsistent or erroneous data, required careful handling. How the Technical Challenges were Solved Developed custom parsing algorithms capable of handling variability in PDF document formats, ensuring consistent and accurate extraction of financial data. Implemented optimized file handling techniques to efficiently process large volumes of PDF files, minimizing processing time and resource utilization. Employed rigorous data cleaning and validation routines to identify and rectify inconsistencies or errors in the extracted data, ensuring its integrity and accuracy. Business Impact Streamlined financial data processing for Toyota dealerships, resulting in improved operational efficiency and decision-making. Enhanced data accuracy and reliability facilitated better insights into dealership performance and financial health. Reduced manual effort and processing time, enabling stakeholders to focus on strategic tasks rather than mundane data processing activities. Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy\n",
      "Objective: The goal of this project is to build an Expert Advisor (EA) Robot that automates trading by using predictions generated from a machine learning (ML) model. The EA will operate in real-time, leveraging both historical and live data to make buy/sell decisions. Data can be obtained via two primary methods: API : A free version that provides limited data for a single currency pair, and a paid version offering access to multiple pairs. Trading Platforms : FX Pro or XM, which will serve as direct sources for real-time trading data. Additionally, the EA Robot will use an MQL5 script that defines the trading strategy, enabling automated trading on any MetaTrader 5 (MT5) platform. The core focus is to train an ML model for predictive trading, integrate data streams, and implement a scalable MQL5 strategy for execution. Key Steps: 1. Project Setup API and Platform Access : Obtain API credentials for both free and paid versions. Set up FX Pro or XM trading platforms and retrieve login details. Ensure compatibility with MetaTrader 5 (MT5) for seamless integration. Test API Connection : Test API endpoints to verify data retrieval for currency pairs. This ensures real-time data flow for both historical and live trading. 2. Data Retrieval and Preparation API Data Integration : Set up API connections to retrieve historical and live trading data. The free version will provide access to data for a single currency pair (e.g., EURUSD), while the paid version allows for multiple pairs (e.g., GBPUSD, GOLD, USDCAD, etc.). FX Pro / XM Data Handling : Integrate FX Pro and XM platforms for data reading and trade execution. Use Python and MT5 libraries to retrieve real-time data for selected currency pairs. Historical Data Storage : Collect historical data for model training, storing it in a structured format (e.g., CSV) for further processing. 3. Model Development and Training Feature Engineering : Compute technical indicators (e.g., RSI, MACD, EMA) for selected currency pairs using historical data. Generate buy/sell signals for training, ensuring the data reflects actual market patterns. Model Training : Develop and train the model using historical data to predict buy/sell signals by choosing appropriate ML model like ( XGBoost, LSTM, or Reinforcement Learning) Focus on ensuring that the model can generalize well to unseen data and respond effectively to market changes. Model Evaluation : Evaluate model performance based on metrics like accuracy, precision, recall, and profit optimization. Adjust and fine-tune hyperparameters for improved predictive performance. 4. Backtesting and Performance Evaluation Backtesting Framework Setup: Develop a system to test the model on historical data. Performance Metrics Definition: Define relevant metrics (e.g., Sharpe ratio, drawdown). Strategy Backtesting: Run the model through historical data to evaluate performance. Results Analysis: Analyze backtesting results and identify areas for improvement. 5. Real-Time Data Integration Real-Time Data Handling : Set up continuous data feeds via APIs or FX Pro/XM platforms, fetching real-time trading data at regular intervals (e.g., 15 minutes). Ensure that the data is cleaned, preprocessed, and normalized on the fly for live prediction purposes. Technical Indicators Calculation : Compute technical indicators on real-time data, ensuring the model uses up-to-date market conditions. Prediction Script : Develop scripts to apply the trained model to real-time data and generate buy/sell predictions. 6. MQL5 Script Development Strategy Implementation : Develop an MQL5 script that includes the trading strategy based on model predictions are creates an interface between the AI system and trading platform. The script will incorporate logic for stop-loss, take-profit, and position sizing, ensuring that risk management is integrated. Platform Compatibility : Ensure the MQL5 script is compatible with any MT5 platform for easy deployment. Execution of Trades : Implement a system where the EA uses the strategy to execute trades automatically based on the model’s predictions.  This approach ensures the development of a robust, data-driven EA Robot that integrates ML predictions with real-time trading platforms and APIs. It also guarantees flexibility for live trading on various currency pairs and adaptability to changing market conditions. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy\n",
      "Client Background Client: A leading IT & tech firm in the USA Industry Type: IT Products & Services: IT Consulting, IT Support, SaaS, Marketing Strategy Organization Size: 10+ The Problem Building AI and ML based YouTube analytics and content creation tool that will help youtuber to understand their subscriber’s watching behaviour, help them in content research, creation and publication. Our Solution Created a MERN stack web application and integrated AI models to helps youtuber to generated titles, descriptions, tags, hashtags, captions etc. Help them to check thumbnail quality, analysis on the videos using video auditor tool, analysis on comments using sentiments analysis, help to under their subscribers using churn predication AI model. Solution Architecture https://www.figma.com/file/WQs01mmmNBZ1SjNE2IV8Sl/Youtube-Web-App-By-SHiV?type=design&node-id=0-1&mode=design&t=Lh2jRx4bGQq6l4WU-0 Deliverables Web Applications Supports Maintenance Feature Enhancement Tech Stack Tools used VS code Language/techniques used React.js Express.js Node.js Python Models used Python libraries Skills used Data scientise Full Stack developer Databases used MongoDB Web Cloud Servers used Google Cloud Platform Project Snapshots Home Page Tool Page Dashboard Blog Page Single Blog Post About Us Contact Us Login Page Title and Description tool Page Thumbnail Quality check tool Project website url https://tubetool.ai Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy\n",
      "Client Background Client: A leading hospital chain in the USA Industry Type: Healthcare Products & Services: Healthcare solutions Organization Size: 200+ The Problem Build a web application. Develop , deploy and maintain the system in the background, and there will be more hospitals partnering with our organization utilizing this tool per our service offering, which could lead to long-term working contracts with us, if interested. The current project is a HIPPA-compliant SPA Web Application that will interface with hospital data dashboards involving discharged patients. Call-Navigator utilizes an SFTP server that is hosted in AWS, this allows our partner hospital to store patient discharge files directly to an S3 bucket. The application and dashboard are currently up and running and pulling information and data from our partner hospital. However, the project was rushed through initial development and there are several front-end features that need to be addressed (or fine-tuned/upgraded) and functions added (search/flag options and redirection) to make the application more user-friendly (dashboard accuracy). There are deliverables such as reporting that is part of the project scope that was not developed but are considered a priority after the features and functionality are improved. According to the OG developer, a GitHub account for our organization will need to be created and code uploaded to two separate projects – one for backend and one for frontend. Let me know if this project would be something you would like to further discuss. Thank you so much for your time and consideration. More information from the OG developer follows: Developer should specifically know (or be able to learn) these specific AWS features since they will be responsible for deployment and maintaining the system. For deployment, the developer must:  modify AWS security groups and target groups, etc…  push code to product **DO NOT REMOVE OG DEVELOPER ACCESS/NETWORK CONNECTION** Currently, there are GitHub actions created that will deploy the code if you set up the proper secrets, GitHub actions on FE/Angular app with deploy code to our static S3 website as well as creations invalidations for CloudFront that bypasses the caching. An ssh config link and key file are available. Need to be able to pull reports on call info by unit, by caller, by diagnosis, physician or across the hospital, specifically on flags for medications, conditions, ect. – Flags and marked issues also need to be represented on the Facility Dashboard page. – Automate the import process of file from hospital. – Set up or ability to set up Hospital employees to log in with their Blessing Credentials (single sign on). – Flags thrown need to show up somewhere in Call Navigator and also be able to go into them and view patients and issues. Right now, the only notification and way to view this is strictly through email. – Ability to hover over icons on Facility Dashboard and see info and be able to click on info and have a page with that info pop up. – On the Discharge tab, when setting a filter, we want that filter to stay set unless we change it. Right now, if you set a filter and then go into a patient, when you come back out, the filter is not set anymore and you have to set all over again. Our Solution – Need to be able to pull reports on call info by unit, by caller, by diagnosis, physician or across the hospital, specifically on flags for medications, conditions, ect. – Flags and marked issues also need to be represented on the Facility Dashboard page. – Automate the import process of file from hospital. – Set up or ability to set up Hospital employees to log in with their Blessing Credentials (single sign on). – Flags thrown need to show up somewhere in Call Navigator and also be able to go into them and view patients and issues. Right now, the only notification and way to view this is strictly through email. – Ability to hover over icons on Facility Dashboard and see info and be able to click on info and have a page with that info pop up. – On the Discharge tab, when setting a filter, we want that filter to stay set unless we change it. Right now, if you set a filter and then go into a patient, when you come back out, the filter is not set anymore and you have to set all over again. Also, let me know if there is a “scope of project” document that we need to complete to help you and your team develop a project plan and timeline. Hopefully, with the above notes and attached files, you’ll be able to put together a project plan and proposal for this project. Deliverables Fully functional application product Maintenance and support Tech Stack Tools used VS Code Language/techniques used Angular Node.js Express.js Databases used MySQL Web Cloud Servers used AWS Project Snapshots Project website url https://callsnavigator.com Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy\n",
      "Client Background Client: A leading IT & tech firm in the USA Industry Type: IT Products & Services: IT Consulting, IT Support, SaaS Organization Size: 200+ The Problem Create a ROASing dashboard to fetch google ads budget spent data using google ads api(campaign-wise). The challenge is to develop a ROAS (Return on Ad Spend) dashboard that efficiently retrieves and displays Google Ads budget spent data on a campaign-wise basis using the Google Ads API. The current system lacks a streamlined method for tracking and analyzing ad spend across various campaigns, leading to difficulties in assessing performance and optimizing budget allocation. The goal is to create a comprehensive dashboard that accurately fetches and visualizes budget data, enabling more effective analysis and decision-making. This involves integrating with the Google Ads API, ensuring data accuracy, and providing clear, actionable insights for better campaign management. 4o mini Our Solution Get Data source access a. Google ads accounts Get Access to google cloud platform a. Bigquery b. VM Setup Google Big Query Data warehouse Design db schema / models Develop Python ETL tool to pull data from source and save it to the data warehouse Deploy ETL tool to VM and run it as cron to update data at db frequently a. Check frequency to run b. Recommended 3 to 6 times a day c. The ETL tool should run automatically at the set intervals and update the db d. Add slack integration module for notifications on Cronjob failure Get Design Access of the Dashboard Implement the Dashboard a. Use Vuexy React Template b. List KPI to display in UI c. List Charts to display in the UI Implement Backend API needed to build the dashboard Solution Architecture https://drive.google.com/file/d/1ygYpSWiOQnREeOMAn4Zk5taNbWv8eaN8/view?usp=sharing Deliverables ETL Tool Dashboard Documentations Source codes Support and Maintenance Tech Stack Tools used VS Code Digital Ocean Language/techniques used Python Google Big Query Google Cloud Platform Google Ads API React.js Node.js Github Models used Python libraries (Pandas) Skills used Data Scientist Full Stack developer Databases used MySQL Web Cloud Servers used Digital Ocean What are the technical Challenges Faced during Project Execution Getting the Google Ads campaigns data How the Technical Challenges were Solved Used the google Ads APIs Project Snapshots Project website url https://roasing.com Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy\n",
      "Client Background Client: A leading automobile & tech firm in the USA Industry Type: Automobiles Products & Services: Manufacturing & Dealership, Financial Services Organization Size: 200+ The Problem The client, a Toyota dealership management firm, faced significant challenges in efficiently processing and analyzing financial data extracted from PDF files. These documents contained crucial information regarding sales, expenses, and other financial metrics across various departments, but parsing and extracting this data accurately proved to be a daunting task. The primary issues included inconsistency in PDF formatting, difficulty in table extraction, and ensuring data integrity throughout the processing pipeline. Our Solution To address these challenges, we developed a comprehensive solution tailored specifically for parsing financial data from Toyota dealership PDF documents. Our solution comprised a series of modular components, each designed to handle specific aspects of the data processing pipeline. We utilized advanced PDF parsing libraries like pdfplumber to extract tables and metadata accurately. Additionally, we implemented custom algorithms for data cleaning and validation to ensure the integrity and accuracy of the extracted data. Solution Architecture The architecture of our solution was designed with modularity and scalability in mind. It consisted of the following key components: PDF Parsing Module: Responsible for extracting tables and metadata from PDF documents using pdfplumber. Data Cleaning and Validation Module: Implemented custom algorithms to clean and validate the extracted data, ensuring consistency and accuracy. Data Aggregation and Analysis Module: Utilized pandas for aggregating and analyzing financial metrics across different departments and time periods. MongoDB Integration: Stored structured financial data in MongoDB collections for efficient storage and retrieval. Deliverables Custom Python scripts for PDF parsing and data processing tailored for Toyota dealership documents. Structured financial data stored in MongoDB collections, ensuring easy access and retrieval. Comprehensive documentation detailing system architecture, usage guidelines, and maintenance procedures. Tech Stack Tools used pdfplumber, pandas, MongoDB Language/techniques used Python, data cleaning, aggregation Models used Custom parsing algorithms Skills used Data processing, Python programming Databases used MongoDB Web Cloud Servers used GCP What are the technical Challenges Faced during Project Execution Variability in PDF document formats: Different Toyota dealership documents exhibited varying formatting styles, making consistent parsing challenging. Handling large volumes of PDF files: Processing a large number of PDF files efficiently without compromising performance was a significant challenge. Ensuring data consistency and accuracy: Maintaining data integrity throughout the processing pipeline, especially in the presence of inconsistent or erroneous data, required careful handling. How the Technical Challenges were Solved Developed custom parsing algorithms capable of handling variability in PDF document formats, ensuring consistent and accurate extraction of financial data. Implemented optimized file handling techniques to efficiently process large volumes of PDF files, minimizing processing time and resource utilization. Employed rigorous data cleaning and validation routines to identify and rectify inconsistencies or errors in the extracted data, ensuring its integrity and accuracy. Business Impact Streamlined financial data processing for Toyota dealerships, resulting in improved operational efficiency and decision-making. Enhanced data accuracy and reliability facilitated better insights into dealership performance and financial health. Reduced manual effort and processing time, enabling stakeholders to focus on strategic tasks rather than mundane data processing activities. Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy\n",
      "Client Background Client: A leading IT & tech firm in the USA Industry Type: IT Products & Services: IT Consulting, IT Support, SaaS Organization Size: 200+ The Problem Transfer an SQL relational/pedigree database that has child-parent across ~500,000 records. Client was interested in 1) transforming it into a Neo4j graph DB 2)Setup CRUD operation of adding,editing,deletion of horses 3)Calculate Coefficient of inbreeding 4)Setting up complete end to end graph database management system 5)Setting up cron jobs which updates coidiff, coi every month and cron job which cleanse the data 6)Query for tail female, mtdna, broodmareSire property Our Solution Data Migration to Neo4j: Analyze the existing SQL database schema to understand the relationships between entities. Design a corresponding graph schema in Neo4j that effectively represents the pedigree data. Develop scripts or use ETL tools to transfer data from the SQL database to Neo4j, ensuring data integrity and consistency during the migration process. CRUD Operations Implementation: Implement Create, Read, Update, and Delete operations for managing horse records in the Neo4j graph database. Develop APIs or user interfaces to interact with the database, allowing users to add, edit, and delete horse records as needed. Coefficient of Inbreeding Calculation: Design algorithms to calculate the COI for each horse based on its pedigree information stored in the graph database. Implement these algorithms using Cypher queries or integrate them into the application code to automate COI calculation for all horses. End-to-End Graph Database Management System: Set up the Neo4j database environment, ensuring proper configuration, security, and scalability. Establish monitoring and logging mechanisms to track database performance and detect any issues proactively. Develop backup and disaster recovery strategies to ensure data availability and integrity. Scheduled Jobs for Updates and Data Cleansing: Configure cron jobs or scheduling mechanisms to run monthly updates for COI calculation and COI differences (COIdiff). Implement data cleansing routines to identify and remove duplicate, outdated, or inconsistent records from the graph database. Query Implementation for Specific Properties: Develop Cypher queries to retrieve tail female, mtDNA, and broodmare sire properties from the graph database. Optimize queries for performance and efficiency, considering the size of the dataset and the complexity of relationships. Solution Architecture Deliverables Neo4j graph database containing migrated pedigree data with CRUD operations implemented. Automated COI calculation and scheduled updates for COIdiff and data cleansing tasks. Comprehensive documentation covering database schema, CRUD operations, COI calculation algorithms, system setup, and maintenance procedures. User guides and tutorials for interacting with the graph database and executing specific queries. Training sessions for client personnel on using and maintaining the system effectively. Tech Stack Tools used Auradb,gcp Language/techniques used Python,cypher query,Neo4j Skills used Cypher Querying Databases used graph database Web Cloud Servers used GCP What are the technical Challenges Faced during Project Execution During the execution of the project, several technical challenges may arise, particularly when dealing with the migration to a graph database, implementation of CRUD operations, COI calculation, setting up scheduled jobs, and query optimization. Some of these challenges include: 1. Data Mapping and Schema Design: – Translating the relational schema of the SQL database into an efficient graph schema for Neo4j may be challenging, especially when dealing with complex relationships and hierarchies present in pedigree data. 2. Data Migration and Integrity: – Ensuring the accuracy and completeness of data migration from the SQL database to Neo4j while maintaining data integrity can be technically challenging. Handling large volumes of data during the migration process may also require optimizations to prevent performance issues. 3. CRUD Operations Implementation: – Implementing CRUD operations in a graph database like Neo4j requires a different approach compared to traditional SQL databases. Ensuring efficient data retrieval, update, and deletion while preserving graph relationships can be challenging. 4. Coefficient of Inbreeding Calculation: – Developing algorithms to calculate the COI based on pedigree data stored in the graph database requires a deep understanding of genetics and graph traversal techniques. Optimizing the COI calculation process for performance and accuracy can pose technical challenges. 5. Database Management and Optimization: – Setting up and managing a Neo4j database environment involves configuring parameters, optimizing queries, and monitoring performance. Ensuring scalability, security, and high availability while minimizing downtime can be technically challenging. 6. Scheduled Jobs and Automation: – Configuring cron jobs or scheduling mechanisms to automate tasks such as COI calculation, COIdiff updates, and data cleansing requires careful planning and implementation. Ensuring the reliability and correctness of scheduled jobs in a production environment can be challenging. 7. Query Optimization: – Writing efficient Cypher queries to retrieve specific properties like tail female, mtDNA, and broodmare sire from the graph database requires optimization techniques such as index usage, query planning, and query rewriting. Balancing query performance with data consistency and complexity can be challenging. 8. Integration with Machine Learning (ML) Models: – If the project involves the development of ML models for data analysis or prediction, integrating these models with the graph database and ensuring seamless data flow between them can be technically challenging. Handling real-time data updates and model inference can also pose challenges. Addressing these technical challenges requires a combination of domain knowledge, expertise in graph database technologies like Neo4j, proficiency in query optimization techniques, and robust software engineering practices. Close collaboration between database administrators, developers, data scientists, and domain experts is essential to overcome these challenges effectively during project execution. How the Technical Challenges were Solved To overcome the technical challenges faced during the project execution, the following strategies and approaches can be adopted: 1. Data Mapping and Schema Design: – Collaborate with domain experts to understand the intricacies of pedigree data and design a graph schema in Neo4j that accurately represents the relationships between entities. – Use Neo4j’s data modeling best practices and guidelines to optimize the schema for efficient data retrieval and traversal. 2. Data Migration and Integrity: – Develop robust ETL (Extract, Transform, Load) processes or use specialized migration tools to transfer data from the SQL database to Neo4j. – Implement data validation checks and reconciliation procedures to ensure the accuracy and integrity of migrated data. 3. CRUD Operations Implementation: – Utilize Neo4j’s Cypher query language and official drivers to implement CRUD operations efficiently. – Leverage Neo4j’s transaction support to ensure data consistency and atomicity during CRUD operations. 4. Coefficient of Inbreeding Calculation: – Collaborate with geneticists or domain experts to design and validate algorithms for COI calculation based on pedigree data. – Implement COI calculation algorithms using Cypher queries or integrate them into the application code, optimizing for performance and accuracy. 5. Database Management and Optimization: – Follow Neo4j’s best practices for database setup, configuration, and optimization. – Monitor database performance using built-in tools or third-party monitoring solutions and fine-tune configuration parameters as needed. 6. Scheduled Jobs and Automation: – Used cron jobsavailable in the programming language/framework used for the project to schedule tasks like COI calculation, COIdiff updates, and data cleansing. – Implement error handling and logging mechanisms to ensure the reliability and correctness of scheduled jobs. 7. Query Optimization: – Profile and analyze Cypher queries to identify performance bottlenecks and optimize them using techniques like query planning, index usage, and query rewriting. – Leverage Neo4j’s query execution plans and profiling tools to identify optimization opportunities. 8. Integration with Machine Learning (ML) Models: – Develop APIs or services to integrate ML models with the graph database, enabling seamless data exchange between them. – Implement real-time data pipelines or batch processing workflows to feed data to ML models and ingest predictions or insights back into the database. Throughout the project execution, maintain close collaboration between database administrators, developers, data scientists, and domain experts to address technical challenges effectively. Conduct regular reviews and iterations to refine solutions and ensure alignment with project goals and requirements. Business Impact The successful execution of the project can lead to significant business impact across several dimensions: 1. Enhanced Data Management: – Migrating the SQL relational database to a graph database like Neo4j enables more efficient and intuitive representation of pedigree data. This improves data accessibility, query performance, and scalability, leading to better overall data management. 2. Improved Operational Efficiency: – Implementing CRUD operations and automation of tasks such as COI calculation and data cleansing streamlines the management of pedigree data. This reduces manual effort, minimizes errors, and enhances operational efficiency. 3. Better Decision Making: – Access to accurate and up-to-date pedigree information, along with calculated COI values, empowers stakeholders to make informed breeding decisions. This can lead to improved breeding outcomes, such as healthier offspring and desired traits, ultimately enhancing the competitiveness of the business. 4. Cost Savings: – By automating routine tasks and optimizing database performance, the project can result in cost savings associated with labor, maintenance, and infrastructure. Additionally, better breeding decisions based on COI calculations can help avoid costly genetic issues in the long term. 5. Competitive Advantage: – Leveraging advanced technologies like Neo4j and machine learning for pedigree management positions the business at the forefront of innovation in the equine industry. This can differentiate the business from competitors and attract customers who value data-driven breeding practices. 6. Compliance and Risk Management: – Maintaining accurate pedigree records and ensuring data integrity through automated validation and cleansing processes helps mitigate regulatory compliance risks. It also reduces the risk of breeding-related issues such as genetic disorders or inbreeding depression, safeguarding the reputation of the business. 7. Scalability and Growth: – The scalable architecture of Neo4j and the automation of key processes lay the foundation for handling larger volumes of pedigree data and supporting business growth. As the business expands, the graph database can accommodate increasing data complexity and user demands. 8. Customer Satisfaction: – Providing stakeholders with easy access to comprehensive pedigree information and tools for informed decision-making enhances customer satisfaction. Breeders, owners, and other stakeholders benefit from a more transparent and efficient breeding process, fostering long-term relationships with the business. Overall, the successful execution of the project can drive significant business impact by optimizing data management processes, improving decision-making capabilities, reducing costs, and strengthening the business’s competitive position in the equine industry. Project Snapshots Project website url Velox.Horse (velox-dev-ezvwl7dg6a-uc.a.run.app) Project Video https://www.loom.com/share/bb2cf781db30489b84d2d539c4b09e7e?sid=9a408b46-b860-42f2-8f62-ac5ff2a772c0 Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy\n",
      "Client Background Client: A leading IT & tech firm in the USA Industry Type: IT Products & Services: IT Consulting, IT Support, SaaS Organization Size: 200+ The Problem To improve the accuracy to 90%+ where the current models which were built were giving an accuracy of 58%. The existing predictive models are currently achieving an accuracy of 58%, which is insufficient for meeting the desired performance benchmarks. The objective is to enhance the accuracy of these models to exceed 90%. This requires a comprehensive evaluation and improvement of the model development process, including data quality, feature engineering, algorithm selection, and hyperparameter tuning. The challenge lies in identifying and implementing effective strategies to significantly boost the model’s predictive accuracy while ensuring robustness and generalizability. Our Solution Our solution architecture is designed to efficiently preprocess financial data, perform feature selection, and train a GRU (Gated Recurrent Unit) model for predictive analysis. The architecture comprises the following components: Data Preprocessing: Involves loading the dataset from a CSV file, checking for null values, and performing initial exploratory data analysis (EDA). Feature Selection: Utilizes correlation analysis to identify relevant features and drop irrelevant ones. Model Building: Constructs a GRU model using TensorFlow’s Keras API, consisting of multiple GRU layers followed by dropout regularization and a dense output layer with a sigmoid activation function. Model Evaluation: Evaluates the trained model on test data to measure its performance. Solution Architecture Deliverables Custom Python scripts for data preprocessing, feature selection, and model building. Trained GRU model saved to a file for future use. Tech Stack Tools used pandas, scikit-learn, TensorFlow, Keras, seaborn, matplotlib Language/techniques used <Python programming, data preprocessing, feature selection, deep learning Models used GRU (Gated Recurrent Unit) Skills used Data preprocessing, feature engineering, model building, evaluation Databases used Multiplexer What are the technical Challenges Faced during Project Execution Handling data variability: Dealing with varying data formats and distributions across different datasets. Feature selection: Identifying relevant features and discarding irrelevant ones to improve model performance. Model optimization: Tuning hyperparameters and optimizing the architecture of the GRU model for better accuracy and generalization. How the Technical Challenges were Solved The solution provides a reliable framework for analyzing financial data and making predictions, aiding in strategic decision-making for the business. By accurately predicting financial trends and performance metrics, the solution enables proactive measures to be taken to optimize operations and maximize profitability. Business Impact The solution provides a reliable framework for analyzing financial data and making predictions, aiding in strategic decision-making for the business. By accurately predicting financial trends and performance metrics, the solution enables proactive measures to be taken to optimize operations and maximize profitability. Project Snapshots Project Video https://www.loom.com/share/bed67661fa7540d2ab39705096c4591d?sid=91b0bdb7-ca4b-43ab-8d81-aaff7ed4a679 Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m url_id \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURL_ID\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 5\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m      6\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m     article_div \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd-post-content tagdiv-type\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sings\\anaconda3\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sings\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sings\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\sings\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 746\u001b[0m     r\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32mc:\\Users\\sings\\anaconda3\\Lib\\site-packages\\requests\\models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(CONTENT_CHUNK_SIZE)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sings\\anaconda3\\Lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\sings\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:1057\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[1;32m-> 1057\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\sings\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:1206\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1203\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_chunk_length()\n\u001b[0;32m   1207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1208\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sings\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:1125\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1125\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m   1126\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\sings\\anaconda3\\Lib\\socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sings\\anaconda3\\Lib\\ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\sings\\anaconda3\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _, row in df.iterrows():\n",
    "    url = row['URL']\n",
    "    url_id = row['URL_ID']\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        article_div = soup.find('div', class_='td-post-content tagdiv-type')\n",
    "        article_text = article_div.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        # Remove underscores\n",
    "        article_text = article_text.replace('_', '')\n",
    "        print(article_text)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction Completed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
