{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Excel File\n",
    "df = pd.read_excel('./data/input.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "URL_ID    147\n",
       "URL       147\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Articles Directory\n",
    "output_dir = 'articles'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://insights.blackcoffer.com/development-of-ea-robot-for-automated-trading/'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = df.iloc[4]['URL']\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if the website is Scrappable 200-OK\n",
    "response = requests.get('https://insights.blackcoffer.com/securing-sensitive-financial-data-with-privacy-preserving-machine-learning-for-predictive-analytics/')\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Amazon Buy Bot, an Automation AI tool to Auto-Checkouts'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = soup.find('h1', class_='entry-title').get_text(strip=True)\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Client Background Client: A leading consulting firm in the USA Industry Type: Consulting Services: Management consultant Organization Size: 100+ Project Objective The main objective of this project is to build the automation tool to buy product on amazon. Project Description This project is basically completed using selenium and Python. All we have done is write a python script for automation using Selenium. Make some clicks use logics to check item is in stock or not. If the item is in stock then it buys the product otherwise repeat the process again. Our Solution A simple python code which uses selenium web driver to do all work. Project Deliverables Python Code Tools used Selenium Webdriver Language/techniques used Python Skills used Web Scraping Selenium Project Snapshots'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_div = soup.find('div', class_='td-post-content tagdiv-type')\n",
    "article_text = article_div.get_text(separator=' ', strip=True) if article_div else \"No Content\"\n",
    "article_text = article_text.replace('_', '')\n",
    "article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Client Background Client Background Client:A leading consulting firm in the USA Client: Industry Type:Consulting Industry Type: Services:Management consultant Services: Organization Size:100 Organization Size: Project Objective Project Objective The main objective of this project is to build the automation tool to buy product on amazon. Project Description Project Description This project is basically completed using selenium and Python. All we have done is write a python script for automation using Selenium. Make some clicks use logics to check item is in stock or not. If the item is in stock then it buys the product otherwise repeat the process again. Our Solution Our Solution A simple python code which uses selenium web driver to do all work. Project Deliverables Project Deliverables Python Code Python Code Tools used Tools used Selenium Webdriver Selenium Webdriver Languagetechniques used Languagetechniques used Python Python Skills used Skills used Web Scraping Selenium Project Snapshots Project Snapshots                         '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove anything that is not a letter, number, space, or punctuation\n",
    "import re\n",
    "content_elements = article_div.find_all()  # Get both <h1> and <p> tags\n",
    "article_text = ' '.join([element.get_text(strip=True) for element in content_elements])\n",
    "\n",
    "article_text = re.sub(r'[^a-zA-Z0-9\\s\\.,!?;:()\\-]', '', article_text)  # Keep letters, numbers, spaces, and common punctuation\n",
    "article_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client Background Client: A leading consulting firm in the USA Industry Type: Consulting Services: Management consultant Organization Size: 100+ Project Objective The main objective of this project is to build the automation tool to buy product on amazon. Project Description This project is basically completed using selenium and Python. All we have done is write a python script for automation using Selenium. Make some clicks use logics to check item is in stock or not. If the item is in stock then it buys the product otherwise repeat the process again. Our Solution A simple python code which uses selenium web driver to do all work. Project Deliverables Python Code Tools used Selenium Webdriver Language/techniques used Python Skills used Web Scraping Selenium Project Snapshots\n"
     ]
    }
   ],
   "source": [
    "# Extract the article text from the div and removed extra line that was coming in form of \"__________________________\"\n",
    "if article_div:\n",
    "    article_text = article_div.get_text(separator=' ', strip=True)\n",
    "    # Remove underscores\n",
    "    article_text = article_text.replace('_', '')\n",
    "else:\n",
    "    article_text = \"No Content\"\n",
    "\n",
    "print(article_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article Netclan20241017 to articles\\Netclan20241017.txt\n"
     ]
    }
   ],
   "source": [
    "article_div = soup.find('div', class_='td-post-content tagdiv-type')\n",
    "article_text = article_div.get_text(separator=' ', strip=True) if article_div else \"No Content\"\n",
    "article_text = article_text.replace('_', '')\n",
    "article_text\n",
    "\n",
    "content = f\"{title}\\n\\n{article_text}\"\n",
    "\n",
    "# Save to a text file named after URL_ID\n",
    "file_name = os.path.join('articles', f\"{df.iloc[0]['URL_ID']}.txt\")\n",
    "with open(file_name, 'w', encoding='utf-8') as file:\n",
    "    file.write(content)\n",
    "\n",
    "print(f\"Saved article {df.iloc[0]['URL_ID']} to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article Netclan20241017 to articles\\Netclan20241017.txt\n",
      "Saved article Netclan20241018 to articles\\Netclan20241018.txt\n",
      "Saved article Netclan20241019 to articles\\Netclan20241019.txt\n",
      "Saved article Netclan20241020 to articles\\Netclan20241020.txt\n",
      "Saved article Netclan20241021 to articles\\Netclan20241021.txt\n",
      "Saved article Netclan20241022 to articles\\Netclan20241022.txt\n",
      "Saved article Netclan20241023 to articles\\Netclan20241023.txt\n",
      "Saved article Netclan20241024 to articles\\Netclan20241024.txt\n",
      "Saved article Netclan20241025 to articles\\Netclan20241025.txt\n",
      "Saved article Netclan20241026 to articles\\Netclan20241026.txt\n",
      "Saved article Netclan20241027 to articles\\Netclan20241027.txt\n",
      "Saved article Netclan20241028 to articles\\Netclan20241028.txt\n",
      "Saved article Netclan20241029 to articles\\Netclan20241029.txt\n",
      "Saved article Netclan20241030 to articles\\Netclan20241030.txt\n",
      "Saved article Netclan20241031 to articles\\Netclan20241031.txt\n",
      "Saved article Netclan20241032 to articles\\Netclan20241032.txt\n",
      "Saved article Netclan20241033 to articles\\Netclan20241033.txt\n",
      "Saved article Netclan20241034 to articles\\Netclan20241034.txt\n",
      "Saved article Netclan20241035 to articles\\Netclan20241035.txt\n",
      "Saved article Netclan20241036 to articles\\Netclan20241036.txt\n",
      "Saved article Netclan20241037 to articles\\Netclan20241037.txt\n",
      "Saved article Netclan20241038 to articles\\Netclan20241038.txt\n",
      "Saved article Netclan20241039 to articles\\Netclan20241039.txt\n",
      "Saved article Netclan20241040 to articles\\Netclan20241040.txt\n",
      "Saved article Netclan20241041 to articles\\Netclan20241041.txt\n",
      "Saved article Netclan20241042 to articles\\Netclan20241042.txt\n",
      "Saved article Netclan20241043 to articles\\Netclan20241043.txt\n",
      "Saved article Netclan20241044 to articles\\Netclan20241044.txt\n",
      "Saved article Netclan20241045 to articles\\Netclan20241045.txt\n",
      "Saved article Netclan20241046 to articles\\Netclan20241046.txt\n",
      "Saved article Netclan20241047 to articles\\Netclan20241047.txt\n",
      "Saved article Netclan20241048 to articles\\Netclan20241048.txt\n",
      "Saved article Netclan20241049 to articles\\Netclan20241049.txt\n",
      "Saved article Netclan20241050 to articles\\Netclan20241050.txt\n",
      "Saved article Netclan20241051 to articles\\Netclan20241051.txt\n",
      "Saved article Netclan20241052 to articles\\Netclan20241052.txt\n",
      "Saved article Netclan20241053 to articles\\Netclan20241053.txt\n",
      "Saved article Netclan20241054 to articles\\Netclan20241054.txt\n",
      "Saved article Netclan20241055 to articles\\Netclan20241055.txt\n",
      "Saved article Netclan20241056 to articles\\Netclan20241056.txt\n",
      "Saved article Netclan20241057 to articles\\Netclan20241057.txt\n",
      "Saved article Netclan20241058 to articles\\Netclan20241058.txt\n",
      "Saved article Netclan20241059 to articles\\Netclan20241059.txt\n",
      "Saved article Netclan20241060 to articles\\Netclan20241060.txt\n",
      "Saved article Netclan20241061 to articles\\Netclan20241061.txt\n",
      "Saved article Netclan20241062 to articles\\Netclan20241062.txt\n",
      "Saved article Netclan20241063 to articles\\Netclan20241063.txt\n",
      "Saved article Netclan20241064 to articles\\Netclan20241064.txt\n",
      "Saved article Netclan20241065 to articles\\Netclan20241065.txt\n",
      "Saved article Netclan20241066 to articles\\Netclan20241066.txt\n",
      "Saved article Netclan20241067 to articles\\Netclan20241067.txt\n",
      "Saved article Netclan20241068 to articles\\Netclan20241068.txt\n",
      "Saved article Netclan20241069 to articles\\Netclan20241069.txt\n",
      "Saved article Netclan20241070 to articles\\Netclan20241070.txt\n",
      "Saved article Netclan20241071 to articles\\Netclan20241071.txt\n",
      "Saved article Netclan20241072 to articles\\Netclan20241072.txt\n",
      "Saved article Netclan20241073 to articles\\Netclan20241073.txt\n",
      "Saved article Netclan20241074 to articles\\Netclan20241074.txt\n",
      "Saved article Netclan20241075 to articles\\Netclan20241075.txt\n",
      "Saved article Netclan20241076 to articles\\Netclan20241076.txt\n",
      "Saved article Netclan20241077 to articles\\Netclan20241077.txt\n",
      "Saved article Netclan20241078 to articles\\Netclan20241078.txt\n",
      "Saved article Netclan20241079 to articles\\Netclan20241079.txt\n",
      "Saved article Netclan20241080 to articles\\Netclan20241080.txt\n",
      "Saved article Netclan20241081 to articles\\Netclan20241081.txt\n",
      "Saved article Netclan20241082 to articles\\Netclan20241082.txt\n",
      "Saved article Netclan20241083 to articles\\Netclan20241083.txt\n",
      "Saved article Netclan20241084 to articles\\Netclan20241084.txt\n",
      "Saved article Netclan20241085 to articles\\Netclan20241085.txt\n",
      "Saved article Netclan20241086 to articles\\Netclan20241086.txt\n",
      "Saved article Netclan20241087 to articles\\Netclan20241087.txt\n",
      "Saved article Netclan20241088 to articles\\Netclan20241088.txt\n",
      "Saved article Netclan20241089 to articles\\Netclan20241089.txt\n",
      "Saved article Netclan20241090 to articles\\Netclan20241090.txt\n",
      "Saved article Netclan20241091 to articles\\Netclan20241091.txt\n",
      "Saved article Netclan20241092 to articles\\Netclan20241092.txt\n",
      "Saved article Netclan20241093 to articles\\Netclan20241093.txt\n",
      "Saved article Netclan20241094 to articles\\Netclan20241094.txt\n",
      "Saved article Netclan20241095 to articles\\Netclan20241095.txt\n",
      "Saved article Netclan20241096 to articles\\Netclan20241096.txt\n",
      "Saved article Netclan20241097 to articles\\Netclan20241097.txt\n",
      "Saved article Netclan20241098 to articles\\Netclan20241098.txt\n",
      "Saved article Netclan20241099 to articles\\Netclan20241099.txt\n",
      "Saved article Netclan20241100 to articles\\Netclan20241100.txt\n",
      "Saved article Netclan20241101 to articles\\Netclan20241101.txt\n",
      "Saved article Netclan20241102 to articles\\Netclan20241102.txt\n",
      "Saved article Netclan20241103 to articles\\Netclan20241103.txt\n",
      "Saved article Netclan20241104 to articles\\Netclan20241104.txt\n",
      "Saved article Netclan20241105 to articles\\Netclan20241105.txt\n",
      "Saved article Netclan20241106 to articles\\Netclan20241106.txt\n",
      "Saved article Netclan20241107 to articles\\Netclan20241107.txt\n",
      "Saved article Netclan20241108 to articles\\Netclan20241108.txt\n",
      "Saved article Netclan20241109 to articles\\Netclan20241109.txt\n",
      "Saved article Netclan20241110 to articles\\Netclan20241110.txt\n",
      "Saved article Netclan20241111 to articles\\Netclan20241111.txt\n",
      "Saved article Netclan20241112 to articles\\Netclan20241112.txt\n",
      "Saved article Netclan20241113 to articles\\Netclan20241113.txt\n",
      "Saved article Netclan20241114 to articles\\Netclan20241114.txt\n",
      "Saved article Netclan20241115 to articles\\Netclan20241115.txt\n",
      "Saved article Netclan20241116 to articles\\Netclan20241116.txt\n",
      "Saved article Netclan20241117 to articles\\Netclan20241117.txt\n",
      "Saved article Netclan20241118 to articles\\Netclan20241118.txt\n",
      "Saved article Netclan20241119 to articles\\Netclan20241119.txt\n",
      "Saved article Netclan20241120 to articles\\Netclan20241120.txt\n",
      "Saved article Netclan20241121 to articles\\Netclan20241121.txt\n",
      "Saved article Netclan20241122 to articles\\Netclan20241122.txt\n",
      "Saved article Netclan20241123 to articles\\Netclan20241123.txt\n",
      "Saved article Netclan20241124 to articles\\Netclan20241124.txt\n",
      "Saved article Netclan20241125 to articles\\Netclan20241125.txt\n",
      "Saved article Netclan20241126 to articles\\Netclan20241126.txt\n",
      "Saved article Netclan20241127 to articles\\Netclan20241127.txt\n",
      "Saved article Netclan20241128 to articles\\Netclan20241128.txt\n",
      "Saved article Netclan20241129 to articles\\Netclan20241129.txt\n",
      "Saved article Netclan20241130 to articles\\Netclan20241130.txt\n",
      "Saved article Netclan20241131 to articles\\Netclan20241131.txt\n",
      "Saved article Netclan20241132 to articles\\Netclan20241132.txt\n",
      "Saved article Netclan20241133 to articles\\Netclan20241133.txt\n",
      "Saved article Netclan20241134 to articles\\Netclan20241134.txt\n",
      "Saved article Netclan20241135 to articles\\Netclan20241135.txt\n",
      "Saved article Netclan20241136 to articles\\Netclan20241136.txt\n",
      "Saved article Netclan20241137 to articles\\Netclan20241137.txt\n",
      "Saved article Netclan20241138 to articles\\Netclan20241138.txt\n",
      "Saved article Netclan20241139 to articles\\Netclan20241139.txt\n",
      "Saved article Netclan20241140 to articles\\Netclan20241140.txt\n",
      "Saved article Netclan20241141 to articles\\Netclan20241141.txt\n",
      "Saved article Netclan20241142 to articles\\Netclan20241142.txt\n",
      "Saved article Netclan20241143 to articles\\Netclan20241143.txt\n",
      "Saved article Netclan20241144 to articles\\Netclan20241144.txt\n",
      "Saved article Netclan20241145 to articles\\Netclan20241145.txt\n",
      "Saved article Netclan20241146 to articles\\Netclan20241146.txt\n",
      "Saved article Netclan20241147 to articles\\Netclan20241147.txt\n",
      "Saved article Netclan20241148 to articles\\Netclan20241148.txt\n",
      "Saved article Netclan20241149 to articles\\Netclan20241149.txt\n",
      "Saved article Netclan20241150 to articles\\Netclan20241150.txt\n",
      "Saved article Netclan20241151 to articles\\Netclan20241151.txt\n",
      "Saved article Netclan20241152 to articles\\Netclan20241152.txt\n",
      "Saved article Netclan20241153 to articles\\Netclan20241153.txt\n",
      "Saved article Netclan20241154 to articles\\Netclan20241154.txt\n",
      "Saved article Netclan20241155 to articles\\Netclan20241155.txt\n",
      "Saved article Netclan20241156 to articles\\Netclan20241156.txt\n",
      "Saved article Netclan20241157 to articles\\Netclan20241157.txt\n",
      "Saved article Netclan20241158 to articles\\Netclan20241158.txt\n",
      "Saved article Netclan20241159 to articles\\Netclan20241159.txt\n",
      "Saved article Netclan20241160 to articles\\Netclan20241160.txt\n",
      "Saved article Netclan20241161 to articles\\Netclan20241161.txt\n",
      "Saved article Netclan20241162 to articles\\Netclan20241162.txt\n",
      "Saved article Netclan20241163 to articles\\Netclan20241163.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.exceptions import Timeout, RequestException\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "\n",
    "session = requests.Session()\n",
    "retry_strategy = Retry(\n",
    "    total=3,  \n",
    "    backoff_factor=1,  # Multiplier for delay between retries: 1s, 2s, 4s, etc.\n",
    "    status_forcelist=[429, 500, 502, 503, 504], \n",
    "    allowed_methods=[\"GET\"] \n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "session.mount(\"http://\", adapter) # Allowing requests from http and https only\n",
    "session.mount(\"https://\", adapter)\n",
    "\n",
    "# Process each URL in the DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    url = row['URL']\n",
    "    url_id = row['URL_ID']\n",
    "    if url:\n",
    "        try:\n",
    "            # Send the request with a timeout\n",
    "            response = session.get(url, timeout=5)\n",
    "            response.raise_for_status() \n",
    "\n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Extract the title (if available)\n",
    "            title_element = soup.find('h1', class_='entry-title')\n",
    "            title = title_element.get_text(strip=True) if title_element else \"No Title\"\n",
    "\n",
    "            # Extract the article content (if available)\n",
    "            article_div = soup.find('div', class_='td-post-content tagdiv-type')\n",
    "            article_text = article_div.get_text(separator=' ', strip=True) if article_div else \"No Content Available\"\n",
    "\n",
    "            # Remove any underscores from the article content\n",
    "            article_text = article_text.replace('_', '')\n",
    "\n",
    "            # Combine title and article content\n",
    "            content = f\"{title}\\n\\n{article_text}\"\n",
    "\n",
    "            # Save to a text file named after URL_ID\n",
    "            file_name = os.path.join('articles', f\"{url_id}.txt\")\n",
    "            with open(file_name, 'w', encoding='utf-8') as file:\n",
    "                file.write(content)\n",
    "\n",
    "            print(f\"Saved article {url_id} to {file_name}\")\n",
    "\n",
    "        except Timeout:\n",
    "            print(f\"Timeout occurred for {url} (ID: {url_id}). Skipping.\")\n",
    "        except RequestException as e:\n",
    "            print(f\"Error fetching {url} (ID: {url_id}): {e}\")\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction Completed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
